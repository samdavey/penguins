{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature and humidity data\n",
    "The data is in separate `.csv` files in the folder `.\\data\\TempHumidData_22_7_2016`.\n",
    "Within the `TempHumidData_22_7_2016` folder are subfolders named `BOX <id number>` and within these are `.csv` files in the following format:\n",
    "```\n",
    "Date/Time,Value,Nest Id,Visit Id\n",
    "8/01/2014 12:54,57.961,117,4\n",
    "8/01/2014 13:24,61.458,117,4\n",
    "8/01/2014 13:54,64.33,117,4\n",
    "```\n",
    "\n",
    "Will be turned into the following format for both the temp and the humidity data:\n",
    "```\n",
    "Date/Time,\n",
    "Value,\n",
    "Nest Id (comes from the folder name),\n",
    "filename of the data csv\n",
    "md5 hash of the record (for identification of duplicates)\n",
    "```\n",
    "Each of these `.csv` files are to be merged into a single data set of all observations for all nests in files named `a_Temp_merged.csv` and `a_Humidity_merged.csv`. (**in progress**)\n",
    "* Files where the filename contains 'ibutton' are excluded.\n",
    "* Files commencing with sensor metadata will have the metadata stripped during the append.\n",
    "\n",
    "At the completion of the appending, duplicate records will be extracted out of the merged file and recorded in files called `b2_Temp_duplicates.csv` and `b2_Humidity_duplicates.csv`. Non-duplicates will be recorded in `b1_Temp_nondupes.csv` and `b1_Humidity_nondupes.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "-- Identifying the Temp and Humidity data files           --\n",
      "------------------------------------------------------------\n",
      "\n",
      "Total files:\t 1728\n",
      "Temp files:\t 697\n",
      "Humidity files:\t 678\n",
      "Bad extension files:\t 4 <- Will be excluded\n",
      "    ..\\data\\TempHumidData_22_7_2016\\BOX 201\\entered into access\\201 HUMID AUG 6 TO OCT 25 2014_with deets.xlsx\n",
      "    ..\\data\\TempHumidData_22_7_2016\\BOX 206\\206 HUMID JAN 11 TO MAR 24 2015\n",
      "    ..\\data\\TempHumidData_22_7_2016\\BOX 219\\219 HUMID JAN 11 TO MAR 24 2015\n",
      "    ..\\data\\TempHumidData_22_7_2016\\BOX 306\\306 notes.xlsx\n",
      "iButton files:\t 348 <- Will be excluded\n",
      "Unrecognised files:\t 1 <- Non-zero is bad\n",
      "    ..\\data\\TempHumidData_22_7_2016\\BOX 218.csv\n",
      "Reconciles:\t True\n"
     ]
    }
   ],
   "source": [
    "# initialise variables\n",
    "folder = os.path.normpath('../data/TempHumidData_22_7_2016')\n",
    "# os.listdir(folder) # proveides the contents of folder as a list\n",
    "\n",
    "# get the list of all files (file_list)\n",
    "# get all the temperature files (temp_files)\n",
    "# get all the humidity files (humidity_files)\n",
    "# add the file to other if it does not contain 'temp' or 'humid'\n",
    "print('------------------------------------------------------------')\n",
    "print('-- Identifying the Temp and Humidity data files           --')\n",
    "print('------------------------------------------------------------')\n",
    "file_list = []\n",
    "temp_files = []\n",
    "humid_files = []\n",
    "ibutton_files = []\n",
    "bad_extensions = []\n",
    "other_files = []\n",
    "for path, subdirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        file_list.append(os.path.join(path, file))\n",
    "        if not file.endswith('.csv'):\n",
    "            bad_extensions.append(os.path.join(path, file))\n",
    "        elif 'ibutton' in file.lower():\n",
    "            ibutton_files.append(os.path.join(path, file))\n",
    "        elif 'temp' in file.lower():\n",
    "            temp_files.append(os.path.join(path, file))\n",
    "        elif 'humid' in file.lower() or 'humd' in file.lower():\n",
    "            humid_files.append(os.path.join(path, file))\n",
    "        else:\n",
    "            other_files.append(os.path.join(path, file))\n",
    "            \n",
    "# status check\n",
    "totals_check = len(file_list) == len(temp_files) + \\\n",
    "    len(humid_files) + \\\n",
    "    len(ibutton_files) + \\\n",
    "    len(bad_extensions) + \\\n",
    "    len(other_files)\n",
    "print()\n",
    "print('Total files:\\t', len(file_list))\n",
    "print('Temp files:\\t', len(temp_files))\n",
    "print('Humidity files:\\t', len(humid_files))\n",
    "print('Bad extension files:\\t', len(bad_extensions), '<- Will be excluded')\n",
    "for file in bad_extensions:\n",
    "    print('   ', file)\n",
    "print('iButton files:\\t', len(ibutton_files), '<- Will be excluded')\n",
    "print('Unrecognised files:\\t', len(other_files), '<- Non-zero is bad')\n",
    "for file in other_files:\n",
    "    print('   ', file)\n",
    "print('Reconciles:\\t', totals_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "-- Merging the Temp files                                 --\n",
      "------------------------------------------------------------\n",
      "\n",
      "Files to merge:\t 697 Commencing setup:\n",
      "Reading ..\\data\\TempHumidData_22_7_2016\\108 TEMP 09 APR TO 03 JUL 14.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 1 fields in line 20, saw 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a00843c31318>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reading'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# make a csv to write the data to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\daveysa1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    496\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\daveysa1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m _parser_defaults = {\n",
      "\u001b[1;32mC:\\Users\\daveysa1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_footer not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\daveysa1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   1603\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0malldata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rows_to_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exclude_implicit_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\daveysa1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_rows_to_cols\u001b[1;34m(self, content)\u001b[0m\n\u001b[0;32m   1978\u001b[0m             msg = ('Expected %d fields in line %d, saw %d' %\n\u001b[0;32m   1979\u001b[0m                    (col_len, row_num + 1, zip_len))\n\u001b[1;32m-> 1980\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1982\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 1 fields in line 20, saw 3"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------------')\n",
    "print('-- Merging the Temp files                                 --')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print('\\nFiles to merge:\\t', len(temp_files), 'Commencing setup:', flush=True)\n",
    "# make a dataframe to hold the output buffer before writing\n",
    "import pandas as pd\n",
    "print('Reading', temp_files[0])\n",
    "# check the header of the file to see if it is a clean csv header or if it has the sensor metadata\n",
    "input_file = open(temp_files[0], 'r')\n",
    "first_line = input_file.\n",
    "df = pd.read_csv(temp_files[0], encoding='utf-8', engine='python')\n",
    "\n",
    "# make a csv to write the data to\n",
    "output = open('a_Temp_merged.csv', newline='')\n",
    "output_excluded = open('a_Temp_merged_excluded.csv', newline='')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# close the files\n",
    "output.close()\n",
    "output_excluded.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
