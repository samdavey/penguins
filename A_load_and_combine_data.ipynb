{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1A Data Preparation\n",
    "This script performs the following tasks:\n",
    "1. Set up the environment\n",
    "2. Load the relevant data sets from file\n",
    " * `NestCharacteristic-Static.csv   -> df_nest_static`\n",
    " * `NestCharacteristic-Seasonal.csv -> df_nest_seasonal`\n",
    " * `BreedingDataCombined.csv        -> df_breeding`\n",
    " * `TempData_2_10_2016.txt          -> df_temp`\n",
    " * `HumidData_2_10_2016.txt         -> df_humd`\n",
    "3. Join them into a) a data file with nests, seasonal coverage and breeding observations and b) a data file for transactional sensor data\n",
    "4. Add additional computed features to the data\n",
    "5. Write the prepared data to file\n",
    " * `SensorDataWithBreedingPhase.csv` contains the temp and humidity logs, together with the breeding phase for that nest.\n",
    " * `NestDataWithBreedingStats.csv` contains all static nest masterdata, together with nest cover and breeding observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment\n",
    "### 1.1 Import the required libraries\n",
    "We need a certain set of common libraries for the tasks to be performed. These are imported below. If an import statement errors, you will need to install the library in your environment using the command line command `pip install <library>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment and variables...\n"
     ]
    }
   ],
   "source": [
    "print('Setting up environment and variables...', flush=True)\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# all the useful and reuseable functions are defined in helper_functions.py\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set up the variables\n",
    "You will need to change the values of the variables below to suit the names and directory location of your files to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:34 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "# update these with your file paths\n",
    "nest_static_file = os.path.normpath('./0_data/NestStaticDataTbl_access_27_01_2017.csv')\n",
    "nest_seasonal_file = os.path.normpath('./0_data/NestSeasonDataTbl_access_27_01_2017.csv')\n",
    "nest_annual_file = os.path.normpath('./0_data/NestAnnualDataTbl_access_28_01_2017.csv')\n",
    "breeding_data_file = os.path.normpath('./0_data/BreedingDataTbl_17_01_2017.csv')\n",
    "temp_humid_file = os.path.normpath('./0_data/TempHumid_all.txt')\n",
    "\n",
    "# write intermediate tables to disk for debugging purposes\n",
    "write_temps = True\n",
    "output_path = os.path.normpath('./output/A_load_and_combine_data')\n",
    "df_sensor_data = None\n",
    "\n",
    "log('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Set up helper functions\n",
    "Most helper functions are in helper_functions.py. These below are required to be in this module so they can use the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_temp_file(df, filepath, df_name):\n",
    "    '''\n",
    "    If write_temps is true, this function will write the specified Pandas dataframe (df) to csv at the specified location (filepath).\n",
    "    Variables:\n",
    "        df: a Pandas dataframe to be written to csv.\n",
    "        filepath: a string in Unix path format (using / not \\) for the csv destination.\n",
    "        df_name: human readable name or description of the dataframe for logging purposes.\n",
    "    '''\n",
    "    if write_temps:\n",
    "        print('{0} - Writing intermediate table {1} to disk.'.format(str(time.ctime()), df_name, filepath), flush=True)\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        df.to_csv(os.path.normpath(filepath))\n",
    "        if os.path.getsize(filepath) > 0:\n",
    "            print('{0} - Written {1}: {2:.3f} MB'.format(str(time.ctime()), filepath, os.path.getsize(filepath)/1000000), flush=True)\n",
    "            \n",
    "def breeding_year(date)   :\n",
    "    '''\n",
    "    Breeding year is defined as 01 Feb to 31 Jan using the year as at 01 Feb.\n",
    "    Arguments:\n",
    "        date is the datetime object representing the observation date to be converted into a breeding year.\n",
    "    Returns:\n",
    "        The Breeding Year for the provided date (as a float, because INTs don't handle NaNs).\n",
    "    '''\n",
    "    if date.month == 1:\n",
    "        return date.year - 1\n",
    "    else:\n",
    "        return date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data from file\n",
    "### 2.1.1 Read in the NestCharacteristic-Static data (df_nest_static)\n",
    "This is the real nest master data to which everything else is joined. Refer to the GitHub Wiki for descriptions of the data fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:34 2017 - Loading the Nest Characteristic (Static) data file.\n",
      "Mon Apr 10 20:50:34 2017 - Nest Characteristic (Static) file is 0.039 MB.\n",
      "Mon Apr 10 20:50:34 2017 - Loading into memory.\n",
      "Mon Apr 10 20:50:34 2017 - Success: loaded 247 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(nest_static_file, 'Nest Characteristic (Static)')\n",
    "data_types = {'nest_id': str,\n",
    "              'nest_type': str,\n",
    "              'shape': str,\n",
    "              'easting': np.float32,\n",
    "              'northing': np.float32,\n",
    "              'elevation': np.float32,\n",
    "              'aspect': np.float32,\n",
    "              'slope': np.float32,\n",
    "              'treatment': str,\n",
    "              'duration_of_insolation': np.float32,\n",
    "              'box_height_mm': np.float32,\n",
    "              'box_length_mm': np.float32,\n",
    "              'box_width_mm': np.float32,\n",
    "              'box_wall_width_mm': np.float32,\n",
    "              'box_lid_depth': np.float32,\n",
    "              'entrance_bearing': np.float32,\n",
    "              'entrance_height': np.float32,\n",
    "              'entrance_length': np.float32,\n",
    "              'entrance_width': np.float32,\n",
    "              'vents': np.float32,\n",
    "              'tunnel': np.float32,\n",
    "              'distance_to_boardwalk_m': np.float32,\n",
    "              'distance_to_landfall_m': np.float32,\n",
    "              'distance_to_shore_vegetation_m': np.float32,\n",
    "              'box_vol_L': np.float32,\n",
    "              'box_area_cm2': np.float32,\n",
    "              'comment': str,\n",
    "              'lat': np.float32,\n",
    "              'long': np.float32,\n",
    "              'autoNumber': np.float32\n",
    "             }\n",
    "df_nest_static = pd.read_csv(nest_static_file, \n",
    "                             header=0,\n",
    "                             dtype=data_types,\n",
    "                             encoding='utf-8',\n",
    "                             error_bad_lines=True,\n",
    "                             warn_bad_lines=True)\n",
    "read_file_handler_end(nest_static_file, 'Nest Characteristic (Static)', df_nest_static, 'df_nest_static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Update and cleanse fields in NestCharacteristic-Static data (df_nest_static)\n",
    "* Make sure all the nest IDs are uppercase and trimmed\n",
    "* Create field `box_vol_L`\n",
    "* Create field `box_area_cm2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:34 2017 - Writing intermediate table df_nest_static to disk.\n",
      "Mon Apr 10 20:50:34 2017 - Written ./output/A_load_and_combine_data/df_nest_static.csv: 0.048 MB\n",
      "Mon Apr 10 20:50:34 2017 - df_nest_static prepared successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the unwanted fields\n",
    "df_nest_static = df_nest_static[['nest_id', 'nest_type', 'shape', 'easting', 'northing', 'elevation',\n",
    "                                 'aspect', 'slope', 'treatment', 'duration_of_insolation',\n",
    "                                 'box_height_mm', 'box_length_mm', 'box_width_mm', 'box_wall_width_mm',\n",
    "                                 'box_lid_depth', 'entrance_bearing', 'entrance_height',\n",
    "                                 'entrance_length', 'entrance_width', 'vents', 'tunnel',\n",
    "                                 'distance_to_boardwalk_m', 'distance_to_landfall_m',\n",
    "                                 'distance_to_shore_vegetation_m', 'comment'\n",
    "                                ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_static['nest_id'] = df_nest_static['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# calc the volume\n",
    "# some boxes have only external measurements, not internal (which we are trying to calc). If it has external\n",
    "# but not internal, then use external measurements\n",
    "def box_vol_L(row):\n",
    "    # box measurements are external, so deduct walls. Boxes have no bottom, so deduct only lid depth from height.\n",
    "    return (row['box_length_mm'] - 2*row['box_wall_width_mm']) * (row['box_width_mm'] - 2*row['box_wall_width_mm']) * (row['box_height_mm']- row['box_lid_depth']) / 1000000\n",
    "df_nest_static['box_vol_L'] = df_nest_static.apply(box_vol_L, axis=1)\n",
    "        \n",
    "# calc the floor area\n",
    "def box_area_cm2(row):\n",
    "    return (row['box_length_mm'] - 2*row['box_wall_width_mm']) * (row['box_width_mm'] - 2*row['box_wall_width_mm']) / 100\n",
    "df_nest_static['box_area_cm2'] = df_nest_static.apply(box_area_cm2, axis=1)\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(os.path.normpath('./output/A_load_and_combine_data')):\n",
    "    os.makedirs('./output/A_load_and_combine_data')\n",
    "write_temp_file(df_nest_static, './output/A_load_and_combine_data/df_nest_static.csv', 'df_nest_static')\n",
    "log('df_nest_static prepared successfully.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Read in the NestCharacteristic-Seasonal data (as df_nest_seasonal)\n",
    "Recorded for old boxes and natural nests. Contains seasonal observations of nest vegetation and cover.\n",
    "New boxes (not recorded) were an experiment in different building methods and their effect on box temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:34 2017 - Loading the Nest Characteristic (Seasonal) data file.\n",
      "Mon Apr 10 20:50:34 2017 - Nest Characteristic (Seasonal) file is 0.120 MB.\n",
      "Mon Apr 10 20:50:34 2017 - Loading into memory.\n",
      "Mon Apr 10 20:50:34 2017 - Success: loaded 1,929 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(nest_seasonal_file, 'Nest Characteristic (Seasonal)')\n",
    "\n",
    "data_types = {'type': str,\n",
    "              'nest_id': str,\n",
    "              'NestSeasYear': str,\n",
    "              'date': str,\n",
    "              'year': str,\n",
    "              'season': str,\n",
    "              'BoxCoverTotal': np.float32,\n",
    "              'BoxCoverDead': np.float32,\n",
    "              'BoxWood': np.float32,\n",
    "              'BoxWoodDead': np.float32,\n",
    "              'BoxVeg': np.float32,\n",
    "              'BoxVegDead': np.float32,\n",
    "              'QuadCoverTotal': np.float32,\n",
    "              'QuadCoverDead': np.float32,\n",
    "              'QuadWood': np.float32,\n",
    "              'QuadWoodDead': np.float32,\n",
    "              'QuadVeg': np.float32,\n",
    "              'QuadVegDead': np.float32,\n",
    "              'cavity_cover': np.float32,\n",
    "              'canopy_cover': np.float32,\n",
    "              'comments': str\n",
    "             }\n",
    "df_nest_seasonal = pd.read_csv(nest_seasonal_file,\n",
    "                               header=0,\n",
    "                               dtype=data_types,\n",
    "                               encoding='utf-8',\n",
    "                               parse_dates=['date'],\n",
    "                               dayfirst=True,\n",
    "                               error_bad_lines=True,\n",
    "                               warn_bad_lines=True)\n",
    "\n",
    "read_file_handler_end(nest_seasonal_file, 'Nest Characteristic (Seasonal)', df_nest_seasonal, 'df_nest_seasonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Update and cleanse fields in the NestCharacteristic-Seasonal data (df_nest_seasonal)\n",
    "* Nest IDs to be all uppercase and trimmed\n",
    "* recalculate the `year` and `season`\n",
    "* create the unique ID `BoxSeasYear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:34 2017 - Writing intermediate table df_nest_seasonal to disk.\n",
      "Mon Apr 10 20:50:34 2017 - Written ./output/A_load_and_combine_data/df_nest_seasonal.csv: 0.158 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted fields: 'year', 'season', 'month',  'NestSeasYear'\n",
    "df_nest_seasonal = df_nest_seasonal[['nest_id', 'BoxCoverTotal', 'BoxCoverDead',\n",
    "                                     'BoxWood', 'BoxWoodDead', 'BoxVeg', 'BoxVegDead', 'QuadCoverTotal',\n",
    "                                     'QuadCoverDead', 'QuadWood', 'QuadWoodDead', 'QuadVeg', 'QuadVegDead',\n",
    "                                     'comments', 'date', 'cavity_cover', 'canopy_cover'\n",
    "                                    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_seasonal['nest_id'] = df_nest_seasonal['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# calculate the breeding year (Feb to Jan)\n",
    "df_nest_seasonal['breeding_year'] = df_nest_seasonal['date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# recalculate season (because was manually created). See helper_functions.py\n",
    "df_nest_seasonal['season'] = df_nest_seasonal['date'].apply(lambda x: season(x))\n",
    "\n",
    "# calc the unique ID\n",
    "df_nest_seasonal['NestSeasYear'] = df_nest_seasonal['nest_id'] + df_nest_seasonal['season'] + df_nest_seasonal['breeding_year'].apply(lambda x: str(x))\n",
    "\n",
    "# all blank canopy_covers should be 0 not NaN\n",
    "df_nest_seasonal['canopy_cover'] = df_nest_seasonal['canopy_cover'].apply(lambda x: 0 if np.isnan(x) else x)\n",
    "\n",
    "write_temp_file(df_nest_seasonal, './output/A_load_and_combine_data/df_nest_seasonal.csv', 'df_nest_seasonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Read in the Annual Nest Data file (as df_nest_annual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:34 2017 - Loading the Nest Characteristic (Annual) data file.\n",
      "Mon Apr 10 20:50:34 2017 - Nest Characteristic (Annual) file is 0.064 MB.\n",
      "Mon Apr 10 20:50:34 2017 - Loading into memory.\n",
      "Mon Apr 10 20:50:35 2017 - Success: loaded 822 records.\n"
     ]
    }
   ],
   "source": [
    "in_file = nest_annual_file\n",
    "desc = 'Nest Characteristic (Annual)'\n",
    "\n",
    "read_file_handler_start(in_file, desc)\n",
    "data_types = {\n",
    "    'nest_id': str,\n",
    "    'year': np.float32,\n",
    "    'nest_year': str,\n",
    "    'is_discrete_bush': str,\n",
    "    'vegetation_cover': np.float32,\n",
    "    'veg_cover_species': str,\n",
    "    'tetragonia': np.float32,\n",
    "    'rhagodia': np.float32,\n",
    "    'acacia': np.float32,\n",
    "    'bush_height': np.float32,\n",
    "    'budh_length': np.float32,\n",
    "    'bush_width': np.float32,\n",
    "    'bush_wall_depth': np.float32,\n",
    "    'cavity_height': np.float32,\n",
    "    'cavity_length': np.float32,\n",
    "    'cavity_width': np.float32,\n",
    "    'cavity_volume': np.float32,\n",
    "    'cavity_area': np.float32,\n",
    "    'entrance_bearing': np.float32,\n",
    "    'entrance_direction': str,\n",
    "    'entrance_height': np.float32,\n",
    "    'entrance_length': np.float32,\n",
    "    'entrance_width': np.float32,\n",
    "    'distance_to_nearest_neighbour_m': np.float32,\n",
    "    'notes': str,\n",
    "    'observation_date': str,\n",
    "}\n",
    "df_nest_annual = pd.read_csv(nest_annual_file,\n",
    "                          header=0, \n",
    "                          dtype=data_types,\n",
    "                          encoding='utf-8',\n",
    "                          parse_dates=['observation_date'],\n",
    "                          dayfirst=True,\n",
    "                          error_bad_lines=True,\n",
    "                          warn_bad_lines=True)\n",
    "\n",
    "read_file_handler_end(in_file, desc, df_nest_annual, 'df_nest_annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:35 2017 - Writing intermediate table df_nest_annual to disk.\n",
      "Mon Apr 10 20:50:35 2017 - Written ./output/A_load_and_combine_data/df_nest_annual.csv: 0.072 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted fields: 'year', 'nest_year', 'veg_cover_species', 'cavity_volume', 'cavity_area',\n",
    "df_nest_annual = df_nest_annual[[\n",
    "        'nest_id', 'is_discrete_bush', 'vegetation_cover',\n",
    "       'tetragonia', 'rhagodia', 'acacia', 'bush_height',\n",
    "       'budh_length', 'bush_width', 'bush_wall_depth', 'cavity_height',\n",
    "       'cavity_length', 'cavity_width', \n",
    "       'entrance_bearing', 'entrance_direction', 'entrance_height',\n",
    "       'entrance_length', 'entrance_width', 'distance_to_nearest_neighbour_m',\n",
    "       'notes', 'observation_date'\n",
    "    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_annual['nest_id'] = df_nest_annual['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# recreate the veg_cover_species\n",
    "def veg_cover_species(row):\n",
    "    result = ''\n",
    "    if not np.isnan(row['tetragonia']) and row['tetragonia'] > 0:\n",
    "        result = 'Tetr '\n",
    "    if not np.isnan(row['rhagodia']) and row['rhagodia'] > 0:\n",
    "        result = '{0}{1}'.format(result, 'Rhag ')\n",
    "    if not np.isnan(row['acacia']) and row['acacia'] > 0:\n",
    "        result = '{0}{1}'.format(result, 'Acac ')\n",
    "    return result\n",
    "df_nest_annual['veg_cover_species'] = df_nest_annual.apply(veg_cover_species, axis=1)\n",
    "\n",
    "# calculate the breeding_year\n",
    "df_nest_annual['breeding_year'] = df_nest_annual['observation_date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# recalculate cavity_area in cm2. L, W, H are in mm.\n",
    "df_nest_annual['cavity_area_cm2'] = df_nest_annual['cavity_length'] * df_nest_annual['cavity_width'] / 100\n",
    "\n",
    "# recalculate cavity_volume in L, assumed a half ellipsoid. Vol of half ellipsoid is 1/2 * 3/4 * pi * abc where a,b,c are the radii\n",
    "df_nest_annual['cavity_volume_L'] = round((0.5 * 0.75 * np.pi * \n",
    "                                     (df_nest_annual['cavity_length']/2) * \n",
    "                                     (df_nest_annual['cavity_width']/2) * \n",
    "                                     (df_nest_annual['cavity_height']/2)\n",
    "                                    ) / 1000000, ndigits=2)\n",
    "\n",
    "write_temp_file(df_nest_annual, './output/A_load_and_combine_data/df_nest_annual.csv', 'df_nest_annual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Read in the BreedingDataCombined file (as df_breeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:35 2017 - Loading the Breeding data file.\n",
      "Mon Apr 10 20:50:35 2017 - Breeding file is 0.711 MB.\n",
      "Mon Apr 10 20:50:35 2017 - Loading into memory.\n",
      "Mon Apr 10 20:50:38 2017 - Success: loaded 16,608 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(breeding_data_file, 'Breeding')\n",
    "data_types = {'nest_id': str,\n",
    "              'observation_date': str,\n",
    "              'Year': str,\n",
    "              'Month': str,\n",
    "              'ActivityStatus': np.float32,\n",
    "              'adult': np.float32,\n",
    "              'clutch': np.float32,\n",
    "              'eggs': np.float32,\n",
    "              'ChicksAlive': np.float32, # running obs, ignore\n",
    "              'ChicksDead': np.float32, # use sum\n",
    "              'TotalChicksHatch': np.float32, # use max\n",
    "              'ChicksAge': np.float32, # use max\n",
    "              'ChicksFledge': np.float32, # use max\n",
    "              'ChicksMissing': np.float32, # ignore\n",
    "              'ContentsNotVisible': np.float32, # ignore\n",
    "              'EggLayDate': str, # use max, avg or min\n",
    "              'IDChick1': np.float32,\n",
    "              'MassChick1': np.float32, # use max\n",
    "              'IDChick2': np.float32,\n",
    "              'MassChick2': np.float32, # use max\n",
    "              'comments': str\n",
    "             }\n",
    "df_breeding = pd.read_csv(breeding_data_file,\n",
    "                          header=0, \n",
    "                          dtype=data_types,\n",
    "                          encoding='utf-8',\n",
    "                          parse_dates=['observation_date', 'EggLayDate'],\n",
    "                          dayfirst=True,\n",
    "                          error_bad_lines=True,\n",
    "                          warn_bad_lines=True)\n",
    "read_file_handler_end(breeding_data_file, 'Breeding', df_breeding, 'df_breeding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Update and cleanse fields in the Breeding data (df_breeding)\n",
    "* Nest IDs to be all uppercase and trimmed\n",
    "* `year` is year of `observation_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:38 2017 - Writing intermediate table df_breeding to disk.\n",
      "Mon Apr 10 20:50:38 2017 - Written ./output/A_load_and_combine_data/df_breeding.csv: 0.970 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted columns: 'Year', 'Month', 'ChicksMissing', 'ContentsNotVisible', \n",
    "df_breeding = df_breeding[[\n",
    "       'nest_id', 'observation_date', 'ActivityStatus',\n",
    "       'adult', 'clutch', 'eggs', 'ChicksDead', 'ChicksAlive', \n",
    "       'TotalChicksHatch', 'ChicksAge', 'ChicksFledge', \n",
    "       'EggLayDate', 'comments', 'IDChick1',\n",
    "       'MassChick1', 'IDChick2', 'MassChick2'\n",
    "    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_breeding['nest_id'] = df_breeding['nest_id'].apply(lambda x: str(x)).apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# create year field\n",
    "df_breeding['breeding_year'] = df_breeding['observation_date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# convert the ActivityStatus into separate columns:\n",
    "# 0 - no activity at all\n",
    "# 1 - some evidence of nesting activity\n",
    "# 2 - breeding initiated (egg laid)\n",
    "# 3 - moulting activity\n",
    "df_breeding['used_for_nesting'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_breeding['used_for_breeding'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==2 else 0)\n",
    "df_breeding['used_for_moulting'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==3 else 0)\n",
    "df_breeding['used_for_nesting_or_breeding'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==1 or x==2 else 0)\n",
    "\n",
    "write_temp_file(df_breeding, './output/A_load_and_combine_data/df_breeding.csv', 'df_breeding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Aggregate the Breeding data to get annual stats\n",
    "* **nest_id**\n",
    "* **breeding_year**\n",
    "* **clutch**\n",
    "* clutch_count\n",
    "* egg_count\n",
    "* chick_count\n",
    "* fletch_count\n",
    "* lay_date\n",
    "* age_at_fledging\n",
    "* mass_at_fledging_chick1\n",
    "* mass_at_fledging_chick2\n",
    "* chick_id1\n",
    "* chick_id2\n",
    "\n",
    "Add field:\n",
    "* `flag_activity_status`: True iff max(ActivityStatus) in year > 0. Note that ActivityStatus was not recorded for the numeric nest_ids, so this field should not be used for 'usage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:38 2017 - Aggregating breeding data to get annual stats.\n",
      "Mon Apr 10 20:50:38 2017 - Writing intermediate table df_clutch_count to disk.\n",
      "Mon Apr 10 20:50:38 2017 - Written ./output/A_load_and_combine_data/df_clutch_count.csv: 0.015 MB\n",
      "Mon Apr 10 20:50:38 2017 - Writing intermediate table df_breeding_gb to disk.\n",
      "Mon Apr 10 20:50:38 2017 - Written ./output/A_load_and_combine_data/df_breeding_gb.csv: 0.015 MB\n"
     ]
    }
   ],
   "source": [
    "log('Aggregating breeding data to get annual stats.')\n",
    "\n",
    "# get the clutches per nest and year\n",
    "# [[chosen columns]] -> groupby -> apply max -> add suffix -> remove multi-index\n",
    "df_clutch_count = df_breeding[['nest_id', 'breeding_year', 'clutch']\n",
    "                             ].groupby(['nest_id', 'breeding_year']).max().add_suffix('_count').reset_index()\n",
    "\n",
    "# get the annual stats per nest, year and clutch\n",
    "temp = df_breeding[['nest_id', 'breeding_year', 'ActivityStatus', 'clutch', 'eggs', 'ChicksDead', 'TotalChicksHatch',\n",
    "                    'ChicksFledge', 'EggLayDate', 'ChicksAge', 'MassChick1', 'MassChick2'\n",
    "                   ]].copy()\n",
    "aggregation = {\n",
    "    'eggs': {'egg_count': 'max'},\n",
    "    'ChicksDead': {'num_deaths': 'sum'}, \n",
    "    'TotalChicksHatch': {'num_hatched': 'max'},\n",
    "    'ChicksFledge': {'num_fledged': 'max'}, \n",
    "    'EggLayDate': {'EggLayDate': 'max'}, \n",
    "    'ChicksAge': {'age_at_fledge': 'max'},\n",
    "    'MassChick1': {'peak_mass_chick1': 'max'},\n",
    "    'MassChick2': {'peak_mass_chick2': 'max'}    \n",
    "}\n",
    "df_breeding_gb = temp.groupby(['nest_id', 'breeding_year', 'clutch']).agg(aggregation).reset_index()\n",
    "# df_breeding_gb is now a multi-index, which is a pain. Lose the top level with droplevel(), which loses the first 3 column names\n",
    "# Get them back by explicitly renaming.\n",
    "df_breeding_gb.columns = df_breeding_gb.columns.droplevel(0)\n",
    "df_breeding_gb.columns.values[0] = 'nest_id'\n",
    "df_breeding_gb.columns.values[1] = 'breeding_year'\n",
    "df_breeding_gb.columns.values[2] = 'clutch'\n",
    "df_breeding_gb['EggLayDate'] = df_breeding_gb['EggLayDate'].apply(lambda x: pd.to_datetime(x)) # confirmed the max EggLayDate is correct when converted back to datetime\n",
    "\n",
    "write_temp_file(df_clutch_count, './output/A_load_and_combine_data/df_clutch_count.csv', 'df_clutch_count')\n",
    "write_temp_file(df_breeding_gb, './output/A_load_and_combine_data/df_breeding_gb.csv', 'df_breeding_gb')\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Load the combined temperature and humidity data\n",
    "Read the temperature and humidity data file into memory and report on success/failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:38 2017 - Loading the Temperature and Humidity data file.\n",
      "Mon Apr 10 20:50:38 2017 - Temperature and Humidity file is 112.663 MB.\n",
      "Mon Apr 10 20:50:38 2017 - Loading into memory. Please be patient.\n",
      "Mon Apr 10 20:50:51 2017 - Success: loaded 2,505,456 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(temp_humid_file, 'Temperature and Humidity')\n",
    "column_names = ['recnum', 'datetime', 'nest_id', 'humidity', 'temp_c']\n",
    "data_types = {'recnum': np.int32, \n",
    "              'datetime': str, \n",
    "              'nest_id': str,\n",
    "              'humidity': np.float32,\n",
    "              'temp_c': np.float32\n",
    "             }\n",
    "df_sensor_data = pd.read_csv(temp_humid_file,\n",
    "                      names=column_names,\n",
    "                      usecols=[0,1,2,3,4],\n",
    "                      dtype=data_types,\n",
    "#                           nrows=10000,\n",
    "                      parse_dates=['datetime'],\n",
    "                      infer_datetime_format=True,\n",
    "                      dayfirst=True,\n",
    "                      encoding='utf-8',\n",
    "                      error_bad_lines=True,\n",
    "                      warn_bad_lines=True\n",
    "                     )\n",
    "read_file_handler_end(temp_humid_file, 'Temperature and Humidity', df_sensor_data, 'df_sensor_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Calculations per-sensor reading\n",
    "The following calculations are added per sensor reading:\n",
    "* The `breeding_year`: same as the calendar year\n",
    "* `temp_bucket` is a category for each 5C temperature range: <0, 0-5, .., 60+\n",
    "* `humidity_bucket`: is a category for roughly 20% humidity ranges, based on human comfort zones\n",
    "* `average_activity_phase`: the average activity conducted at the time of the observation\n",
    "\n",
    "Note: An `actual_activity_phase` (the current phase of breeding based on per-nest observations) is added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:50:51 2017 - Cleanse the nest_ids...\n",
      "Mon Apr 10 20:50:52 2017 -  Done.\n",
      "Mon Apr 10 20:50:52 2017 - Calculating breeding year...\n",
      "Mon Apr 10 20:51:01 2017 -  Done.\n",
      "Mon Apr 10 20:51:01 2017 - Calculating temperature buckets...\n",
      "Mon Apr 10 20:51:05 2017 -  Done.\n",
      "Mon Apr 10 20:51:05 2017 - Calculating humidity buckets...\n",
      "Mon Apr 10 20:51:08 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# make sure the nest IDs are all caps\n",
    "log('Cleanse the nest_ids...')\n",
    "df_sensor_data['nest_id'] = df_sensor_data['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "log(' Done.')\n",
    "\n",
    "# add the breeding_year (same as calendar year): \n",
    "log('Calculating breeding year...')\n",
    "df_sensor_data['breeding_year'] = df_sensor_data['datetime'].apply(lambda x: breeding_year(x))\n",
    "log(' Done.')\n",
    "\n",
    "# Add flags for various temperature ranges. Uses temp_bucket() as defined in helper_functions.py\n",
    "# These are summed to give the amount of time in the temp band\n",
    "log('Calculating temperature buckets...')\n",
    "df_sensor_data['temp_bucket'] = df_sensor_data['temp_c'].apply(temp_bucket)\n",
    "log(' Done.')\n",
    "\n",
    "# # Add flags for various humidity ranges. Uses humidity_bucket() as defined in helper_functions.py\n",
    "# # These are summed to give the amount of time in the humidity band\n",
    "log('Calculating humidity buckets...')\n",
    "df_sensor_data['humidity_bucket'] = df_sensor_data['humidity'].apply(humidity_bucket)\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Join the NestCharacteristic Static and Seasonal data\n",
    "`df_nest_seasonal` + `df_nest_static` -> `df_nest_joined`\n",
    "\n",
    "Seasonal is `left` and Static is `right`, such that the Seasonal data is augmented with the nests static metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:08 2017 - Joining the Nest Characteristic (Seasonal and Static) data sets .\n",
      "Mon Apr 10 20:51:08 2017 - Join complete. Here are the stats:\n",
      "Records in seasonal data:                   1,929\n",
      "Records in static data:                       247\n",
      "                              -------------------\n",
      "Records in joined data:                     1,929\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids:                           241\n",
      "\n",
      "Mon Apr 10 20:51:08 2017 - Writing intermediate table df_nest_joined to disk.\n",
      "Mon Apr 10 20:51:08 2017 - Written ./output/A_load_and_combine_data/df_nest_joined.csv: 0.526 MB\n",
      "Mon Apr 10 20:51:08 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "log('Joining the Nest Characteristic (Seasonal and Static) data sets .')\n",
    "df_nest_joined = pd.merge(left=df_nest_seasonal,\n",
    "                            right=df_nest_static,\n",
    "                            how='left',\n",
    "                            on=['nest_id'], # both have same keys\n",
    "                            left_on=None, # same key names: don't need to specify R and L\n",
    "                            right_on=None, # same key names: don't need to specify R and L\n",
    "                            left_index=False, # dont' use left df index as key\n",
    "                            right_index=False, # dont' use right df index as key\n",
    "                            sort=True, # for efficiency do/not sort the df first\n",
    "                            suffixes=['_seasonal', '_static']\n",
    "                            )\n",
    "if df_nest_joined is not None:\n",
    "    log('Join complete. Here are the stats:')\n",
    "    print('Records in seasonal data:    {0:>20,}'.format(len(df_nest_seasonal)))\n",
    "    print('Records in static data:      {0:>20,}'.format(len(df_nest_static)))\n",
    "    print('                              -------------------')\n",
    "    print('Records in joined data:      {0:>20,}'.format(len(df_nest_joined)))\n",
    "    print('\\nOverview:')\n",
    "    gb = df_nest_joined.groupby(['nest_id'])\n",
    "    print('Number of nest_ids:          {0:>20,}\\n'.format(len(gb)))\n",
    "    write_temp_file(df_nest_joined, './output/A_load_and_combine_data/df_nest_joined.csv', 'df_nest_joined')\n",
    "    del gb\n",
    "else:\n",
    "    log('JOIN FAILED!!!.')\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Join the breeding stats together\n",
    "`df_breeding_gb + df_clutch_count -> df_breeding_annual_stats`\n",
    "\n",
    "Clutch counts per year and annual clutch survival stats.\n",
    "Note that the reduced record count in `df_breeding_annual_stats` compared to `df_clutch_count` is due to a number of nest-years having blank/zero clutches. These are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:08 2017 - Merging the aggregated breeding stats.\n",
      "Mon Apr 10 20:51:08 2017 - Join complete. Here are the stats:\n",
      "Records in annual stats data:                  277\n",
      "Records in clutch count data:                  860\n",
      "                              -------------------\n",
      "Records in joined data:                        277\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids in clutch count:            212\n",
      "Number of nest_ids in breeding stats:          122\n",
      "Number of nest_ids in joined:                  122\n",
      "\n",
      "Mon Apr 10 20:51:09 2017 - Writing intermediate table df_breeding_annual_stats to disk.\n",
      "Mon Apr 10 20:51:09 2017 - Written ./output/A_load_and_combine_data/df_breeding_annual_stats.csv: 0.016 MB\n",
      "Mon Apr 10 20:51:09 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# join the clutch count on to the annual stats\n",
    "log('Merging the aggregated breeding stats.')\n",
    "df_breeding_annual_stats = pd.merge(left=df_breeding_gb,\n",
    "                                    right=df_clutch_count,\n",
    "                                    how='left',\n",
    "                                    on=['nest_id', 'breeding_year'], # both have same keys\n",
    "                                    sort=True # for efficiency do/not sort the df first\n",
    "                                   )\n",
    "\n",
    "log('Join complete. Here are the stats:')\n",
    "print('Records in annual stats data: {0:>20,}'.format(len(df_breeding_gb)))\n",
    "print('Records in clutch count data: {0:>20,}'.format(len(df_clutch_count)))\n",
    "print('                              -------------------')\n",
    "print('Records in joined data:       {0:>20,}'.format(len(df_breeding_annual_stats)))\n",
    "print('\\nOverview:')\n",
    "gb = df_breeding_annual_stats.groupby(['nest_id'])\n",
    "print('Number of nest_ids in clutch count:   {0:>12,}'.format(len(df_clutch_count.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in breeding stats: {0:>12,}'.format(len(df_breeding_gb.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in joined:         {0:>12,}\\n'.format(len(df_breeding_annual_stats.groupby(['nest_id']))))\n",
    "write_temp_file(df_breeding_annual_stats, './output/A_load_and_combine_data/df_breeding_annual_stats.csv', 'df_breeding_annual_stats')\n",
    "del gb\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Join the Nest data (seasonal and static) to the Breeding stats\n",
    "`df_nest_joined + df_breeding_annual_stats -> df_nest_and_breeding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:09 2017 - Merging the aggregated breeding stats to the static and seasonal nest data.\n",
      "Mon Apr 10 20:51:09 2017 - Join complete. Here are the stats:\n",
      "Records in nest data:                        1,929\n",
      "Records in breeding stats data:                277\n",
      "                                      ------------\n",
      "Records in joined data:                      2,066\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids in nest data:               241\n",
      "Number of nest_ids in breeding stats:          122\n",
      "Number of nest_ids in joined:                  241\n",
      "\n",
      "Mon Apr 10 20:51:09 2017 - Writing intermediate table df_nest_and_breeding to disk.\n",
      "Mon Apr 10 20:51:09 2017 - Written ./output/A_load_and_combine_data/NestDataWithBreedingStats.csv: 0.604 MB\n",
      "Mon Apr 10 20:51:09 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "# join the annual clutch and breeding stats onto the full seasonal and static nest data\n",
    "log('Merging the aggregated breeding stats to the static and seasonal nest data.')\n",
    "df_nest_and_breeding = pd.merge(left=df_nest_joined,\n",
    "                                right=df_breeding_annual_stats,\n",
    "                                how='left',\n",
    "                                on=['nest_id','breeding_year'], # both have same keys\n",
    "                                sort=True \n",
    "                               )\n",
    "\n",
    "log('Join complete. Here are the stats:')\n",
    "print('Records in nest data:                 {0:>12,}'.format(len(df_nest_joined)))\n",
    "print('Records in breeding stats data:       {0:>12,}'.format(len(df_breeding_annual_stats)))\n",
    "print('                                      ------------')\n",
    "print('Records in joined data:               {0:>12,}'.format(len(df_nest_and_breeding)))\n",
    "print('\\nOverview:')\n",
    "gb = df_breeding_annual_stats.groupby(['nest_id'])\n",
    "print('Number of nest_ids in nest data:      {0:>12,}'.format(len(df_nest_joined.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in breeding stats: {0:>12,}'.format(len(df_breeding_annual_stats.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in joined:         {0:>12,}\\n'.format(len(df_nest_and_breeding.groupby(['nest_id']))))\n",
    "write_temp_file(df_nest_and_breeding, './output/A_load_and_combine_data/NestDataWithBreedingStats.csv', 'df_nest_and_breeding')\n",
    "del gb\n",
    "log( 'Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Aggregate the sensor data into stats per breeding phase\n",
    "To understand the effect of nest conditions (from sensor data) in the choice of nest and breeding success of the nest, we need to break up the stats into:\n",
    "* *annual stats* which represent the averages, spikes etc for the entire year. These give an understanding of the nest itself.\n",
    "* *phase stats* which represent the conditions during specific phases of the breeding cycle. E.g. during nesting, during incubation, during rearing. To get these phase stats, we need to get the phase boundary dates from the breeding observation data.\n",
    "\n",
    "The nest sensor readings are aggregated to summarise the nest conditions by `nest`, `breeding_year` and `activity_phase`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5.1 Get the actual breeding phase dates\n",
    "Summarise the breeding data to obtain the following:\n",
    "* list of all nests (regardless of breeding activity)\n",
    "* the `nesting_date` for each nest in each year\n",
    "* the `egg_lay_date` for each nest, year and clutch\n",
    "* the `hatch_date` for each nest, year and clutch\n",
    "* the `fledge_date` for each nest, year and clutch\n",
    "\n",
    "Join these all back together to get the phase dates all in one place, then join the combined result on to the sensor data table and calculate the phase in which each sensor reading occurred.\n",
    "This will take a while.\n",
    "\n",
    "**Issue: Nesting dates dont work: the second clutch will have first nesting date and the first obs for many nests is after the lay date, so nesting_date > lay_date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:09 2017 - Calculating the breeding phase dates for each nest and year.\n",
      "Mon Apr 10 20:51:09 2017 - Writing intermediate table df_all_nests to disk.\n",
      "Mon Apr 10 20:51:09 2017 - Written ./output/A_load_and_combine_data/df_all_nests.csv: 0.002 MB\n",
      "Mon Apr 10 20:51:09 2017 -  Creating a table for the courting and egg lay dates...\n",
      "Mon Apr 10 20:51:09 2017 - Writing intermediate table gb_lay_date to disk.\n",
      "Mon Apr 10 20:51:09 2017 - Written ./output/A_load_and_combine_data/gb_lay_date.csv: 0.011 MB\n",
      "Mon Apr 10 20:51:09 2017 -  Done.\n",
      "Mon Apr 10 20:51:09 2017 -  Creating a table for the hatch dates...\n",
      "Mon Apr 10 20:51:09 2017 - Writing intermediate table gb_hatch_date to disk.\n",
      "Mon Apr 10 20:51:09 2017 - Written ./output/A_load_and_combine_data/gb_hatch_date.csv: 0.007 MB\n",
      "Mon Apr 10 20:51:09 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# for each nest, year and clutch, get the following:\n",
    "# first activity_status date (nesting_date), EggLayDate, hatch_date, fledge_date\n",
    "# nesting_date, hatch_date, fledge_date are the min observation_date per nest, year, clutch where the value is not NaN\n",
    "\n",
    "log('Calculating the breeding phase dates for each nest and year.')\n",
    "# take each unique nest id, which we'll join the rest of the data onto\n",
    "df_all_nests = df_nest_static[['nest_id']].drop_duplicates()\n",
    "write_temp_file(df_all_nests, './output/A_load_and_combine_data/df_all_nests.csv', 'df_all_nests')\n",
    "\n",
    "# egg_lay_date\n",
    "log(' Creating a table for the courting and egg lay dates...')\n",
    "gb_lay_date = df_breeding[['nest_id', 'breeding_year', 'clutch', 'EggLayDate']\n",
    "                         ].groupby(['nest_id', 'breeding_year', 'clutch']).min().reset_index()\n",
    "gb_lay_date.rename(columns={'EggLayDate': 'egg_lay_date'}, inplace=True)\n",
    "# nesting date: 31 days before egg_lay_date\n",
    "gb_lay_date['courting_date'] = gb_lay_date['egg_lay_date'] - datetime.timedelta(days=31)\n",
    "write_temp_file(gb_lay_date, './output/A_load_and_combine_data/gb_lay_date.csv', 'gb_lay_date')\n",
    "log(' Done.')\n",
    "\n",
    "# hatch_date\n",
    "log(' Creating a table for the hatch dates...')\n",
    "def hatch_date(row):\n",
    "    return row['observation_date'] - datetime.timedelta(days=row['ChicksAge'])\n",
    "# get the observation date (select columns) where age is not blank (i.e. they're there)\n",
    "gb_hatch_date = df_breeding[['nest_id', 'breeding_year', 'clutch', \n",
    "                             'observation_date', 'ChicksAge']].loc[df_breeding['ChicksAge'].notnull()]\n",
    "gb_hatch_date['hatch_date'] = gb_hatch_date.apply(hatch_date, axis=1)\n",
    "# get the min hatch_date \n",
    "gb_hatch_date = gb_hatch_date[['nest_id', 'breeding_year', 'clutch', 'hatch_date'\n",
    "                              ]].groupby(['nest_id', 'breeding_year', 'clutch']).min().reset_index()\n",
    "write_temp_file(gb_hatch_date, './output/A_load_and_combine_data/gb_hatch_date.csv', 'gb_hatch_date')\n",
    "log(' Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:09 2017 -  Creating a table for the fledge dates...\n",
      "Mon Apr 10 20:51:11 2017 - Writing intermediate table gb_fledge_date to disk.\n",
      "Mon Apr 10 20:51:11 2017 - Written ./output/A_load_and_combine_data/gb_fledge_date.csv: 0.007 MB\n",
      "Mon Apr 10 20:51:11 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# fledge_date\n",
    "log(' Creating a table for the fledge dates...')\n",
    "# is either the date that the chicks were of age and no longer observed in the nest, or were observed dead\n",
    "# get the observation date (select columns) where there is a fledge flag\n",
    "gb_fledge_date = df_breeding[['nest_id', 'breeding_year', 'clutch', 'observation_date', \n",
    "                              'ChicksAlive', 'ChicksDead', 'ChicksFledge']].fillna(0)\n",
    "gb_fledge_date['dead_or_fledged'] = gb_fledge_date.apply(lambda row: row['ChicksFledge'] > 0 \n",
    "                                                         or (row['ChicksDead'] > 0 and row['ChicksAlive'] == 0), axis=1)\n",
    "gb_fledge_date = gb_fledge_date.query('dead_or_fledged')\n",
    "# get the min obs date, which is the earliest fledge recording (per clutch)\n",
    "gb_fledge_date = gb_fledge_date[['nest_id', 'breeding_year', 'clutch', 'observation_date'\n",
    "                                ]].groupby(['nest_id', 'breeding_year', 'clutch']).min().reset_index()\n",
    "# rename the obs date \n",
    "gb_fledge_date.rename(columns={'observation_date': 'dead_or_fledge_date'}, inplace=True)\n",
    "write_temp_file(gb_fledge_date, './output/A_load_and_combine_data/gb_fledge_date.csv', 'gb_fledge_date')\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_phase_dates` contains the full breeding data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:11 2017 -  Merging the phase date tables...\n",
      "Mon Apr 10 20:51:11 2017 - Writing intermediate table df_phase_dates to disk.\n",
      "Mon Apr 10 20:51:11 2017 - Written ./output/A_load_and_combine_data/df_phase_dates.csv: 0.019 MB\n",
      "Mon Apr 10 20:51:11 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# join the key date tables together\n",
    "log(' Merging the phase date tables...')\n",
    "df_phase_dates = pd.merge(left=df_all_nests, \n",
    "                          right=gb_lay_date, \n",
    "                          how='left', \n",
    "                          on=['nest_id'], \n",
    "                          sort=True\n",
    "                         )\n",
    "df_phase_dates = pd.merge(left=df_phase_dates, \n",
    "                          right=gb_hatch_date, \n",
    "                          how='left', \n",
    "                          on=['nest_id', 'breeding_year', 'clutch'], \n",
    "                          sort=True\n",
    "                         )\n",
    "df_phase_dates = pd.merge(left=df_phase_dates, \n",
    "                          right=gb_fledge_date, \n",
    "                          how='left', \n",
    "                          on=['nest_id', 'breeding_year', 'clutch'], \n",
    "                          sort=True\n",
    "                         )\n",
    "df_phase_dates = pd.merge(left=df_phase_dates, \n",
    "                          right=df_clutch_count, \n",
    "                          how='left', \n",
    "                          on=['nest_id', 'breeding_year'], \n",
    "                          sort=True\n",
    "                         )\n",
    "write_temp_file(df_phase_dates, './output/A_load_and_combine_data/df_phase_dates.csv', 'df_phase_dates')\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:11 2017 -  Pivot breeding data to get the clutch dates...\n",
      "Mon Apr 10 20:51:11 2017 - Writing intermediate table df_clutch_pivot to disk.\n",
      "Mon Apr 10 20:51:11 2017 - Written ./output/A_load_and_combine_data/df_clutch_pivot.csv: 0.007 MB\n",
      "Mon Apr 10 20:51:11 2017 -  Done.\n",
      "Mon Apr 10 20:51:11 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "log(' Pivot breeding data to get the clutch dates...')\n",
    "# get the required cols\n",
    "df_clutch_pivot = gb_lay_date[['nest_id', 'breeding_year', 'clutch', 'egg_lay_date']].copy()\n",
    "# we have to combine the index (because pivot() does not like a multi-index) and then split it apart a few steps later\n",
    "df_clutch_pivot['nestyear'] = df_clutch_pivot['nest_id'] + '-' + (df_clutch_pivot['breeding_year'].apply(lambda x: str(int(x))))\n",
    "# drop the old index fields\n",
    "df_clutch_pivot = df_clutch_pivot[['nestyear', 'clutch', 'egg_lay_date']]\n",
    "# do the pivot to get the (up to three) clutch dates per nest and year\n",
    "df_clutch_pivot = df_clutch_pivot.pivot(index='nestyear', columns='clutch')['egg_lay_date'].reset_index()\n",
    "\n",
    "# rename and restore the indexes\n",
    "df_clutch_pivot.rename(columns={1.0: 'clutch_1', 2.0: 'clutch_2', 3.0: 'clutch_3'}, inplace=True)\n",
    "df_clutch_pivot['nest_id'] = df_clutch_pivot['nestyear'].apply(lambda x: x.split('-')[0])\n",
    "df_clutch_pivot['breeding_year'] = df_clutch_pivot['nestyear'].apply(lambda x: float(x.split('-')[1]))\n",
    "df_clutch_pivot = df_clutch_pivot[['nest_id', 'breeding_year', 'clutch_1', 'clutch_2', 'clutch_3']]\n",
    "write_temp_file(df_clutch_pivot, './output/A_load_and_combine_data/df_clutch_pivot.csv', 'df_clutch_pivot')\n",
    "log(' Done.')\n",
    "log('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add clutch dates to the sensor data** \n",
    "\n",
    "The `sensor_data` are lacking a `clutch` number, which will create duplicates if we attempt to join on the phase dates. Get the clutch dates and join them into the `sensor_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:51:11 2017 - Join the clutch dates to the sensor data.\n",
      "Mon Apr 10 20:51:12 2017 - Done. Rows: 2,505,456\n",
      "Mon Apr 10 20:51:12 2017 - Assigning a clutch number to each sensor record. Be patient.\n",
      "Mon Apr 10 20:55:13 2017 - Done.\n",
      "Mon Apr 10 20:55:13 2017 - Writing intermediate table df_sensor_clutch to disk.\n",
      "Mon Apr 10 20:55:45 2017 - Written ./output/A_load_and_combine_data/df_sensor_clutch.csv: 287.631 MB\n"
     ]
    }
   ],
   "source": [
    "# to avoid making epic dupes, we need to first add the clutch number on to the sensor data table\n",
    "\n",
    "log('Join the clutch dates to the sensor data.')\n",
    "# join on to the sensor data\n",
    "df_sensor_clutch = pd.merge(left=df_sensor_data, \n",
    "                            right=df_clutch_pivot, \n",
    "                            how='left', \n",
    "                            on=['nest_id', 'breeding_year'],\n",
    "                            sort=True\n",
    "                           )\n",
    "log('Done. Rows: {0:,}'.format(len(df_sensor_clutch)))\n",
    "\n",
    "log('Assigning a clutch number to each sensor record. Be patient.')\n",
    "# flag each reading with a clutch number\n",
    "def clutch_number(row):\n",
    "    if pd.isnull(row['clutch_1']):\n",
    "        # there are no breeding observations for this nest and year\n",
    "        return 0\n",
    "    else:\n",
    "        # there is at least 1 clutch\n",
    "        if pd.isnull(row['clutch_2']) or row['datetime'] < row['clutch_2']:\n",
    "            # there was only a single clutch, or there were >1 but this reading was before the 2nd clutch\n",
    "            return 1\n",
    "        elif pd.isnull(row['clutch_3']) or (not pd.isnull(row['clutch_3']) and row['datetime'] < row['clutch_3']):\n",
    "            # there is a 2nd clutch if we got this far. if there is no 3rd, or the reading is before the 3rd, then this is 2nd\n",
    "            return 2\n",
    "        else:\n",
    "            # there is a 3rd clutch and the sensor reading is after the 3rd\n",
    "            return 3\n",
    "\n",
    "df_sensor_clutch['clutch_number'] = df_sensor_clutch.apply(lambda row: clutch_number(row), axis=1)\n",
    "log('Done.')\n",
    "write_temp_file(df_sensor_clutch, './output/A_load_and_combine_data/df_sensor_clutch.csv', 'df_sensor_clutch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the actual breeding phase dates on to the sensor data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:55:45 2017 - Join the phase dates on to the sensor data.\n",
      "Mon Apr 10 20:55:47 2017 - Done. Rows: 2,505,456\n"
     ]
    }
   ],
   "source": [
    "log('Join the phase dates on to the sensor data.')\n",
    "df_sensor_phase = pd.merge(left=df_sensor_clutch,\n",
    "                        right=df_phase_dates,\n",
    "                        how='left',\n",
    "                        left_on=['nest_id', 'breeding_year', 'clutch_number'], \n",
    "                        right_on=['nest_id', 'breeding_year', 'clutch'], \n",
    "                        sort=True \n",
    "                        )\n",
    "del df_sensor_phase['clutch_number']\n",
    "log('Done. Rows: {0:,}'.format(len(df_sensor_phase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the breeding_phase for each sensor reading** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 20:55:47 2017 - Calculate the breeding phase for each sensor record... Be patient...\n",
      "Mon Apr 10 20:55:56 2017 -   > Calendar year: done.\n",
      "Mon Apr 10 20:56:05 2017 -   > Month: done.\n",
      "Mon Apr 10 20:56:13 2017 -   > Day: done.\n",
      "Mon Apr 10 20:56:20 2017 -   > Hour: done.\n",
      "Mon Apr 10 20:56:28 2017 -   > Minute: done.\n",
      "Mon Apr 10 20:56:29 2017 -   > Season: done.\n",
      "Mon Apr 10 20:56:37 2017 -   > Season_Year: done.\n",
      "Mon Apr 10 21:04:41 2017 -   > Breeding Phase: done.\n",
      "Mon Apr 10 21:04:41 2017 - Done.\n",
      "Mon Apr 10 21:04:41 2017 - Writing intermediate table df_sensor_phase to disk.\n",
      "Mon Apr 10 21:05:34 2017 - Written ./output/A_load_and_combine_data/df_sensor_phase.csv: 452.662 MB\n"
     ]
    }
   ],
   "source": [
    "log('Calculate the breeding phase for each sensor record... Be patient...')\n",
    "\n",
    "\n",
    "def breeding_phase(row):\n",
    "    '''\n",
    "    Calculates the breeding phase per record based on the phase dates.\n",
    "    There are additional rules when the subsequent phase did not occur:\n",
    "    a) Eggs without a hatch date are assumed dead at 45 days and nest returns to 'Not in use'\n",
    "    b) Hatched eggs without a fledge date are assumed fledged/dead after 90 days\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row: a dataframe record containing the following columns:\n",
    "         'egg_lay_date', \n",
    "         'courting_date',\n",
    "         'hatch_date', \n",
    "         'dead_or_fledge_date'\n",
    "         \n",
    "    Returns:\n",
    "    --------\n",
    "    A string representing the current breeding phase according to the phase dates above.\n",
    "    Return values are:\n",
    "        'Not in use' : Between the dead_or_fledge_date of the previous clutch/breeding season and next courting_date\n",
    "        'Courting',  : Between the courting_date and egg_lay_date\n",
    "        'Incubating',: Between the egg_lay_date and hatch_date\n",
    "        'Rearing'    : Between the hatch_date and dead_or_fledge_date\n",
    "    '''\n",
    "    if row['courting_date'] <= row['datetime'] < row['egg_lay_date']:\n",
    "        return 'Courting'\n",
    "    elif row['egg_lay_date'] <= row['datetime'] < row['hatch_date']:\n",
    "        return 'Incubating'\n",
    "    elif pd.isnull(row['hatch_date']) and (row['egg_lay_date'] <= row['datetime'] < row['egg_lay_date'] \n",
    "                                         + datetime.timedelta(days=45)):\n",
    "        return 'Incubating'\n",
    "    elif row['hatch_date'] <= row['datetime'] < row['dead_or_fledge_date']:\n",
    "        return 'Rearing'\n",
    "    elif pd.isnull(row['dead_or_fledge_date']) and (row['hatch_date'] <= row['datetime'] < row['hatch_date'] \n",
    "                                                + datetime.timedelta(days=90)):\n",
    "        return 'Rearing'\n",
    "    else:\n",
    "        return 'Not in use'\n",
    "\n",
    "\n",
    "def season_from_month(month):\n",
    "    '''\n",
    "    Returns the season (southern hemisphere) for the provided month.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    month (scalar): the float/integer representing the calendar month\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A string representing the season (southern hemisphere) for the calendar month provided. \n",
    "    I.e. 'Spring', 'Summer', 'Autumn', 'Winter'\n",
    "    '''\n",
    "    if 3 <= month <= 5:\n",
    "        return 'Autumn'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'Winter'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'Spring'\n",
    "    elif month == 12 or 1 <= month <= 2:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        raise ValueError('{0} is not a valid month. Expecting 1-12.'.format(month))\n",
    "    \n",
    "\n",
    "def season_year(date):\n",
    "    '''\n",
    "    Returns the season_year for a given year and month. The season_year represents \n",
    "    the year in which the season began and adjusts treats Summer 2013 as including \n",
    "    the following months: December 2013, January 2014, February 2014.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row: a dataframe row that includes a 'breeding_year' and a 'month'\n",
    "    breeding_year: The calendar year for the provided month\n",
    "    month: The calendar month\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    An integer representing the year in which the season began.\n",
    "    '''\n",
    "    if date.month <= 2:\n",
    "        return date.year - 1\n",
    "    else:\n",
    "        return date.year\n",
    "\n",
    "\n",
    "df_sensor_phase['calendar_year'] = df_sensor_phase['datetime'].apply(lambda x: x.year)\n",
    "log('  > Calendar year: done.')\n",
    "df_sensor_phase['month'] = df_sensor_phase['datetime'].apply(lambda x: x.month)\n",
    "log('  > Month: done.')\n",
    "df_sensor_phase['day'] = df_sensor_phase['datetime'].apply(lambda x: x.day)\n",
    "log('  > Day: done.')\n",
    "df_sensor_phase['hour'] = df_sensor_phase['datetime'].apply(lambda x: x.hour)\n",
    "log('  > Hour: done.')\n",
    "df_sensor_phase['minute'] = df_sensor_phase['datetime'].apply(lambda x: x.minute)\n",
    "log('  > Minute: done.')\n",
    "df_sensor_phase['season'] = df_sensor_phase['month'].apply(lambda x: season_from_month(x))\n",
    "log('  > Season: done.')\n",
    "df_sensor_phase['season_year'] = df_sensor_phase['datetime'].apply(lambda x: season_year(x))\n",
    "log('  > Season_Year: done.')\n",
    "df_sensor_phase['breeding_phase'] = df_sensor_phase.apply(breeding_phase, axis=1)\n",
    "log('  > Breeding Phase: done.')\n",
    "log('Done.')\n",
    "\n",
    "write_temp_file(df_sensor_phase, './output/A_load_and_combine_data/df_sensor_phase.csv', 'df_sensor_phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up intermediate tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 21:05:34 2017 - Cleaning up intermediate data tables...\n",
      "Mon Apr 10 21:05:35 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "log('Cleaning up intermediate data tables...')\n",
    "del df_sensor_clutch\n",
    "del gb_lay_date\n",
    "del gb_hatch_date\n",
    "del gb_fledge_date\n",
    "del df_nest_joined\n",
    "del df_breeding_annual_stats\n",
    "del df_clutch_count\n",
    "del df_breeding_gb\n",
    "log('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickle the two key data files for use in later scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 21:08:37 2017 - Writing the final tables to pickle for future use...\n",
      "Mon Apr 10 21:08:43 2017 - Done.\n",
      "Mon Apr 10 21:08:43 2017 - \n",
      "\n",
      "Script complete.\n"
     ]
    }
   ],
   "source": [
    "log('Writing the final tables to pickle for future use...')\n",
    "df_sensor_phase.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_sensor_phase.pkl'))\n",
    "df_nest_and_breeding.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_nest_and_breeding.pkl'))\n",
    "df_nest_static.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_nest_static.pkl'))\n",
    "log('Done.')\n",
    "    \n",
    "log('\\n\\nScript complete.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
