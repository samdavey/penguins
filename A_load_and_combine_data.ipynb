{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1A Data Preparation\n",
    "This script performs the following tasks:\n",
    "1. Set up the environment\n",
    "2. Load the relevant data sets from file\n",
    " * `NestCharacteristic-Static.csv   -> df_nest_static`\n",
    " * `NestCharacteristic-Seasonal.csv -> df_nest_seasonal`\n",
    " * `BreedingDataCombined.csv        -> df_breeding`\n",
    " * `TempData_2_10_2016.txt          -> df_temp`\n",
    " * `HumidData_2_10_2016.txt         -> df_humd`\n",
    "3. Join them into a) a data file with nests, seasonal coverage and breeding observations and b) a data file for transactional sensor data\n",
    "4. Add additional computed features to the data\n",
    "5. Write the prepared data to file\n",
    " * `SensorDataWithBreedingPhase.csv` contains the temp and humidity logs, together with the breeding phase for that nest.\n",
    " * `NestDataWithBreedingStats.csv` contains all static nest masterdata, together with nest cover and breeding observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment\n",
    "### 1.1 Import the required libraries\n",
    "We need a certain set of common libraries for the tasks to be performed. These are imported below. If an import statement errors, you will need to install the library in your environment using the command line command `pip install <library>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment and variables...\n"
     ]
    }
   ],
   "source": [
    "print('Setting up environment and variables...', flush=True)\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# all the useful and reuseable functions are defined in helper_functions.py\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set up the variables\n",
    "You will need to change the values of the variables below to suit the names and directory location of your files to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:39 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "# update these with your file paths\n",
    "nest_static_file = os.path.normpath('./0_data/NestStaticDataTbl_access_27_01_2017.csv')\n",
    "nest_seasonal_file = os.path.normpath('./0_data/NestSeasonDataTbl_access_27_01_2017.csv')\n",
    "nest_annual_file = os.path.normpath('./0_data/NestAnnualDataTbl_access_28_01_2017.csv')\n",
    "breeding_data_file = os.path.normpath('./0_data/BreedingDataTbl_17_01_2017.csv')\n",
    "temp_humid_file = os.path.normpath('./0_data/TempHumid_all.txt')\n",
    "\n",
    "# write intermediate tables to disk for debugging purposes\n",
    "write_temps = True\n",
    "df_sensor_data = None\n",
    "\n",
    "log('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Set up helper functions\n",
    "Most helper functions are in helper_functions.py. These below are required to be in this module so they can use the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_temp_file(df, filepath, df_name):\n",
    "    '''\n",
    "    If write_temps is true, this function will write the specified Pandas dataframe (df) to csv at the specified location (filepath).\n",
    "    Variables:\n",
    "        df: a Pandas dataframe to be written to csv.\n",
    "        filepath: a string in Unix path format (using / not \\) for the csv destination.\n",
    "        df_name: human readable name or description of the dataframe for logging purposes.\n",
    "    '''\n",
    "    if write_temps:\n",
    "        print('{0} - Writing intermediate table {1} to disk.'.format(str(time.ctime()), df_name, filepath), flush=True)\n",
    "        df.to_csv(os.path.normpath(filepath))\n",
    "        if os.path.getsize(filepath) > 0:\n",
    "            print('{0} - Written {1}: {2:.3f} MB'.format(str(time.ctime()), filepath, os.path.getsize(filepath)/1000000), flush=True)\n",
    "            \n",
    "def breeding_year(date)   :\n",
    "    '''\n",
    "    Breeding year is defined as 01 Feb to 31 Jan using the year as at 01 Feb.\n",
    "    Arguments:\n",
    "        date is the datetime object representing the observation date to be converted into a breeding year.\n",
    "    Returns:\n",
    "        The Breeding Year for the provided date (as a float, because INTs don't handle NaNs).\n",
    "    '''\n",
    "    if date.month == 1:\n",
    "        return date.year - 1\n",
    "    else:\n",
    "        return date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data from file\n",
    "### 2.1.1 Read in the NestCharacteristic-Static data (df_nest_static)\n",
    "This is the real nest master data to which everything else is joined. Refer to the GitHub Wiki for descriptions of the data fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:39 2017 - Loading the Nest Characteristic (Static) data file.\n",
      "Tue Apr  4 20:55:39 2017 - Nest Characteristic (Static) file is 0.039 MB.\n",
      "Tue Apr  4 20:55:39 2017 - Loading into memory.\n",
      "Tue Apr  4 20:55:39 2017 - Success: loaded 247 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(nest_static_file, 'Nest Characteristic (Static)')\n",
    "data_types = {'nest_id': str,\n",
    "              'nest_type': str,\n",
    "              'shape': str,\n",
    "              'easting': np.float32,\n",
    "              'northing': np.float32,\n",
    "              'elevation': np.float32,\n",
    "              'aspect': np.float32,\n",
    "              'slope': np.float32,\n",
    "              'treatment': str,\n",
    "              'duration_of_insolation': np.float32,\n",
    "              'box_height_mm': np.float32,\n",
    "              'box_length_mm': np.float32,\n",
    "              'box_width_mm': np.float32,\n",
    "              'box_wall_width_mm': np.float32,\n",
    "              'box_lid_depth': np.float32,\n",
    "              'entrance_bearing': np.float32,\n",
    "              'entrance_height': np.float32,\n",
    "              'entrance_length': np.float32,\n",
    "              'entrance_width': np.float32,\n",
    "              'vents': np.float32,\n",
    "              'tunnel': np.float32,\n",
    "              'distance_to_boardwalk_m': np.float32,\n",
    "              'distance_to_landfall_m': np.float32,\n",
    "              'distance_to_shore_vegetation_m': np.float32,\n",
    "              'box_vol_L': np.float32,\n",
    "              'box_area_cm2': np.float32,\n",
    "              'comment': str,\n",
    "              'lat': np.float32,\n",
    "              'long': np.float32,\n",
    "              'autoNumber': np.float32\n",
    "             }\n",
    "df_nest_static = pd.read_csv(nest_static_file, \n",
    "                             header=0,\n",
    "                             dtype=data_types,\n",
    "                             encoding='utf-8',\n",
    "                             error_bad_lines=True,\n",
    "                             warn_bad_lines=True)\n",
    "read_file_handler_end(nest_static_file, 'Nest Characteristic (Static)', df_nest_static, 'df_nest_static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Update and cleanse fields in NestCharacteristic-Static data (df_nest_static)\n",
    "* Make sure all the nest IDs are uppercase and trimmed\n",
    "* Create field `box_vol_L`\n",
    "* Create field `box_area_cm2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:39 2017 - Writing intermediate table df_nest_static to disk.\n",
      "Tue Apr  4 20:55:39 2017 - Written ./output/A_load_and_combine_data/df_nest_static.csv: 0.048 MB\n",
      "Tue Apr  4 20:55:39 2017 - df_nest_static prepared successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the unwanted fields\n",
    "df_nest_static = df_nest_static[['nest_id', 'nest_type', 'shape', 'easting', 'northing', 'elevation',\n",
    "                                 'aspect', 'slope', 'treatment', 'duration_of_insolation',\n",
    "                                 'box_height_mm', 'box_length_mm', 'box_width_mm', 'box_wall_width_mm',\n",
    "                                 'box_lid_depth', 'entrance_bearing', 'entrance_height',\n",
    "                                 'entrance_length', 'entrance_width', 'vents', 'tunnel',\n",
    "                                 'distance_to_boardwalk_m', 'distance_to_landfall_m',\n",
    "                                 'distance_to_shore_vegetation_m', 'comment'\n",
    "                                ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_static['nest_id'] = df_nest_static['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# calc the volume\n",
    "# some boxes have only external measurements, not internal (which we are trying to calc). If it has external\n",
    "# but not internal, then use external measurements\n",
    "def box_vol_L(row):\n",
    "    # box measurements are external, so deduct walls. Boxes have no bottom, so deduct only lid depth from height.\n",
    "    return (row['box_length_mm'] - 2*row['box_wall_width_mm']) * (row['box_width_mm'] - 2*row['box_wall_width_mm']) * (row['box_height_mm']- row['box_lid_depth']) / 1000000\n",
    "df_nest_static['box_vol_L'] = df_nest_static.apply(box_vol_L, axis=1)\n",
    "        \n",
    "# calc the floor area\n",
    "def box_area_cm2(row):\n",
    "    return (row['box_length_mm'] - 2*row['box_wall_width_mm']) * (row['box_width_mm'] - 2*row['box_wall_width_mm']) / 100\n",
    "df_nest_static['box_area_cm2'] = df_nest_static.apply(box_area_cm2, axis=1)\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(os.path.normpath('./output/A_load_and_combine_data')):\n",
    "    os.makedirs('./output/A_load_and_combine_data')\n",
    "write_temp_file(df_nest_static, './output/A_load_and_combine_data/df_nest_static.csv', 'df_nest_static')\n",
    "log('df_nest_static prepared successfully.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Read in the NestCharacteristic-Seasonal data (as df_nest_seasonal)\n",
    "Recorded for old boxes and natural nests. Contains seasonal observations of nest vegetation and cover.\n",
    "New boxes (not recorded) were an experiment in different building methods and their effect on box temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:39 2017 - Loading the Nest Characteristic (Seasonal) data file.\n",
      "Tue Apr  4 20:55:39 2017 - Nest Characteristic (Seasonal) file is 0.120 MB.\n",
      "Tue Apr  4 20:55:39 2017 - Loading into memory.\n",
      "Tue Apr  4 20:55:40 2017 - Success: loaded 1,929 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(nest_seasonal_file, 'Nest Characteristic (Seasonal)')\n",
    "\n",
    "data_types = {'type': str,\n",
    "              'nest_id': str,\n",
    "              'NestSeasYear': str,\n",
    "              'date': str,\n",
    "              'year': str,\n",
    "              'season': str,\n",
    "              'BoxCoverTotal': np.float32,\n",
    "              'BoxCoverDead': np.float32,\n",
    "              'BoxWood': np.float32,\n",
    "              'BoxWoodDead': np.float32,\n",
    "              'BoxVeg': np.float32,\n",
    "              'BoxVegDead': np.float32,\n",
    "              'QuadCoverTotal': np.float32,\n",
    "              'QuadCoverDead': np.float32,\n",
    "              'QuadWood': np.float32,\n",
    "              'QuadWoodDead': np.float32,\n",
    "              'QuadVeg': np.float32,\n",
    "              'QuadVegDead': np.float32,\n",
    "              'cavity_cover': np.float32,\n",
    "              'canopy_cover': np.float32,\n",
    "              'comments': str\n",
    "             }\n",
    "df_nest_seasonal = pd.read_csv(nest_seasonal_file,\n",
    "                               header=0,\n",
    "                               dtype=data_types,\n",
    "                               encoding='utf-8',\n",
    "                               parse_dates=['date'],\n",
    "                               dayfirst=True,\n",
    "                               error_bad_lines=True,\n",
    "                               warn_bad_lines=True)\n",
    "\n",
    "read_file_handler_end(nest_seasonal_file, 'Nest Characteristic (Seasonal)', df_nest_seasonal, 'df_nest_seasonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Update and cleanse fields in the NestCharacteristic-Seasonal data (df_nest_seasonal)\n",
    "* Nest IDs to be all uppercase and trimmed\n",
    "* recalculate the `year` and `season`\n",
    "* create the unique ID `BoxSeasYear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:40 2017 - Writing intermediate table df_nest_seasonal to disk.\n",
      "Tue Apr  4 20:55:40 2017 - Written ./output/A_load_and_combine_data/df_nest_seasonal.csv: 0.158 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted fields: 'year', 'season', 'month',  'NestSeasYear'\n",
    "df_nest_seasonal = df_nest_seasonal[['nest_id', 'BoxCoverTotal', 'BoxCoverDead',\n",
    "                                     'BoxWood', 'BoxWoodDead', 'BoxVeg', 'BoxVegDead', 'QuadCoverTotal',\n",
    "                                     'QuadCoverDead', 'QuadWood', 'QuadWoodDead', 'QuadVeg', 'QuadVegDead',\n",
    "                                     'comments', 'date', 'cavity_cover', 'canopy_cover'\n",
    "                                    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_seasonal['nest_id'] = df_nest_seasonal['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# calculate the breeding year (Feb to Jan)\n",
    "df_nest_seasonal['breeding_year'] = df_nest_seasonal['date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# recalculate season (because was manually created). See helper_functions.py\n",
    "df_nest_seasonal['season'] = df_nest_seasonal['date'].apply(lambda x: season(x))\n",
    "\n",
    "# calc the unique ID\n",
    "df_nest_seasonal['NestSeasYear'] = df_nest_seasonal['nest_id'] + df_nest_seasonal['season'] + df_nest_seasonal['breeding_year'].apply(lambda x: str(x))\n",
    "\n",
    "# all blank canopy_covers should be 0 not NaN\n",
    "df_nest_seasonal['canopy_cover'] = df_nest_seasonal['canopy_cover'].apply(lambda x: 0 if np.isnan(x) else x)\n",
    "\n",
    "write_temp_file(df_nest_seasonal, './output/A_load_and_combine_data/df_nest_seasonal.csv', 'df_nest_seasonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Read in the Annual Nest Data file (as df_nest_annual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:40 2017 - Loading the Nest Characteristic (Annual) data file.\n",
      "Tue Apr  4 20:55:40 2017 - Nest Characteristic (Annual) file is 0.064 MB.\n",
      "Tue Apr  4 20:55:40 2017 - Loading into memory.\n",
      "Tue Apr  4 20:55:40 2017 - Success: loaded 822 records.\n"
     ]
    }
   ],
   "source": [
    "in_file = nest_annual_file\n",
    "desc = 'Nest Characteristic (Annual)'\n",
    "\n",
    "read_file_handler_start(in_file, desc)\n",
    "data_types = {\n",
    "    'nest_id': str,\n",
    "    'year': np.float32,\n",
    "    'nest_year': str,\n",
    "    'is_discrete_bush': str,\n",
    "    'vegetation_cover': np.float32,\n",
    "    'veg_cover_species': str,\n",
    "    'tetragonia': np.float32,\n",
    "    'rhagodia': np.float32,\n",
    "    'acacia': np.float32,\n",
    "    'bush_height': np.float32,\n",
    "    'budh_length': np.float32,\n",
    "    'bush_width': np.float32,\n",
    "    'bush_wall_depth': np.float32,\n",
    "    'cavity_height': np.float32,\n",
    "    'cavity_length': np.float32,\n",
    "    'cavity_width': np.float32,\n",
    "    'cavity_volume': np.float32,\n",
    "    'cavity_area': np.float32,\n",
    "    'entrance_bearing': np.float32,\n",
    "    'entrance_direction': str,\n",
    "    'entrance_height': np.float32,\n",
    "    'entrance_length': np.float32,\n",
    "    'entrance_width': np.float32,\n",
    "    'distance_to_nearest_neighbour_m': np.float32,\n",
    "    'notes': str,\n",
    "    'observation_date': str,\n",
    "}\n",
    "df_nest_annual = pd.read_csv(nest_annual_file,\n",
    "                          header=0, \n",
    "                          dtype=data_types,\n",
    "                          encoding='utf-8',\n",
    "                          parse_dates=['observation_date'],\n",
    "                          dayfirst=True,\n",
    "                          error_bad_lines=True,\n",
    "                          warn_bad_lines=True)\n",
    "\n",
    "read_file_handler_end(in_file, desc, df_nest_annual, 'df_nest_annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:40 2017 - Writing intermediate table df_nest_annual to disk.\n",
      "Tue Apr  4 20:55:40 2017 - Written ./output/A_load_and_combine_data/df_nest_annual.csv: 0.072 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted fields: 'year', 'nest_year', 'veg_cover_species', 'cavity_volume', 'cavity_area',\n",
    "df_nest_annual = df_nest_annual[[\n",
    "        'nest_id', 'is_discrete_bush', 'vegetation_cover',\n",
    "       'tetragonia', 'rhagodia', 'acacia', 'bush_height',\n",
    "       'budh_length', 'bush_width', 'bush_wall_depth', 'cavity_height',\n",
    "       'cavity_length', 'cavity_width', \n",
    "       'entrance_bearing', 'entrance_direction', 'entrance_height',\n",
    "       'entrance_length', 'entrance_width', 'distance_to_nearest_neighbour_m',\n",
    "       'notes', 'observation_date'\n",
    "    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_annual['nest_id'] = df_nest_annual['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# recreate the veg_cover_species\n",
    "def veg_cover_species(row):\n",
    "    result = ''\n",
    "    if not np.isnan(row['tetragonia']) and row['tetragonia'] > 0:\n",
    "        result = 'Tetr '\n",
    "    if not np.isnan(row['rhagodia']) and row['rhagodia'] > 0:\n",
    "        result = '{0}{1}'.format(result, 'Rhag ')\n",
    "    if not np.isnan(row['acacia']) and row['acacia'] > 0:\n",
    "        result = '{0}{1}'.format(result, 'Acac ')\n",
    "    return result\n",
    "df_nest_annual['veg_cover_species'] = df_nest_annual.apply(veg_cover_species, axis=1)\n",
    "\n",
    "# calculate the breeding_year\n",
    "df_nest_annual['breeding_year'] = df_nest_annual['observation_date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# recalculate cavity_area in cm2. L, W, H are in mm.\n",
    "df_nest_annual['cavity_area_cm2'] = df_nest_annual['cavity_length'] * df_nest_annual['cavity_width'] / 100\n",
    "\n",
    "# recalculate cavity_volume in L, assumed a half ellipsoid. Vol of half ellipsoid is 1/2 * 3/4 * pi * abc where a,b,c are the radii\n",
    "df_nest_annual['cavity_volume_L'] = round((0.5 * 0.75 * np.pi * \n",
    "                                     (df_nest_annual['cavity_length']/2) * \n",
    "                                     (df_nest_annual['cavity_width']/2) * \n",
    "                                     (df_nest_annual['cavity_height']/2)\n",
    "                                    ) / 1000000, ndigits=2)\n",
    "\n",
    "write_temp_file(df_nest_annual, './output/A_load_and_combine_data/df_nest_annual.csv', 'df_nest_annual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Read in the BreedingDataCombined file (as df_breeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:40 2017 - Loading the Breeding data file.\n",
      "Tue Apr  4 20:55:40 2017 - Breeding file is 0.711 MB.\n",
      "Tue Apr  4 20:55:40 2017 - Loading into memory.\n",
      "Tue Apr  4 20:55:44 2017 - Success: loaded 16,608 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(breeding_data_file, 'Breeding')\n",
    "data_types = {'nest_id': str,\n",
    "              'observation_date': str,\n",
    "              'Year': str,\n",
    "              'Month': str,\n",
    "              'ActivityStatus': np.float32,\n",
    "              'adult': np.float32,\n",
    "              'clutch': np.float32,\n",
    "              'eggs': np.float32,\n",
    "              'ChicksAlive': np.float32, # running obs, ignore\n",
    "              'ChicksDead': np.float32, # use sum\n",
    "              'TotalChicksHatch': np.float32, # use max\n",
    "              'ChicksAge': np.float32, # use max\n",
    "              'ChicksFledge': np.float32, # use max\n",
    "              'ChicksMissing': np.float32, # ignore\n",
    "              'ContentsNotVisible': np.float32, # ignore\n",
    "              'EggLayDate': str, # use max, avg or min\n",
    "              'IDChick1': np.float32,\n",
    "              'MassChick1': np.float32, # use max\n",
    "              'IDChick2': np.float32,\n",
    "              'MassChick2': np.float32, # use max\n",
    "              'comments': str\n",
    "             }\n",
    "df_breeding = pd.read_csv(breeding_data_file,\n",
    "                          header=0, \n",
    "                          dtype=data_types,\n",
    "                          encoding='utf-8',\n",
    "                          parse_dates=['observation_date', 'EggLayDate'],\n",
    "                          dayfirst=True,\n",
    "                          error_bad_lines=True,\n",
    "                          warn_bad_lines=True)\n",
    "read_file_handler_end(breeding_data_file, 'Breeding', df_breeding, 'df_breeding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Update and cleanse fields in the Breeding data (df_breeding)\n",
    "* Nest IDs to be all uppercase and trimmed\n",
    "* `year` is year of `observation_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:44 2017 - Writing intermediate table df_breeding to disk.\n",
      "Tue Apr  4 20:55:44 2017 - Written ./output/A_load_and_combine_data/df_breeding.csv: 0.970 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted columns: 'Year', 'Month', 'ChicksMissing', 'ContentsNotVisible', \n",
    "df_breeding = df_breeding[[\n",
    "       'nest_id', 'observation_date', 'ActivityStatus',\n",
    "       'adult', 'clutch', 'eggs', 'ChicksDead', 'ChicksAlive', \n",
    "       'TotalChicksHatch', 'ChicksAge', 'ChicksFledge', \n",
    "       'EggLayDate', 'comments', 'IDChick1',\n",
    "       'MassChick1', 'IDChick2', 'MassChick2'\n",
    "    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_breeding['nest_id'] = df_breeding['nest_id'].apply(lambda x: str(x)).apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# create year field\n",
    "df_breeding['breeding_year'] = df_breeding['observation_date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# convert the ActivityStatus into separate columns:\n",
    "# 0 - no activity at all\n",
    "# 1 - some evidence of nesting activity\n",
    "# 2 - breeding initiated (egg laid)\n",
    "# 3 - moulting activity\n",
    "df_breeding['used_for_nesting'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_breeding['used_for_breeding'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==2 else 0)\n",
    "df_breeding['used_for_moulting'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==3 else 0)\n",
    "df_breeding['used_for_nesting_or_breeding'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==1 or x==2 else 0)\n",
    "\n",
    "write_temp_file(df_breeding, './output/A_load_and_combine_data/df_breeding.csv', 'df_breeding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Aggregate the Breeding data to get annual stats\n",
    "* **nest_id**\n",
    "* **breeding_year**\n",
    "* **clutch**\n",
    "* clutch_count\n",
    "* egg_count\n",
    "* chick_count\n",
    "* fletch_count\n",
    "* lay_date\n",
    "* age_at_fledging\n",
    "* mass_at_fledging_chick1\n",
    "* mass_at_fledging_chick2\n",
    "* chick_id1\n",
    "* chick_id2\n",
    "\n",
    "Add field:\n",
    "* `flag_activity_status`: True iff max(ActivityStatus) in year > 0. Note that ActivityStatus was not recorded for the numeric nest_ids, so this field should not be used for 'usage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:44 2017 - Aggregating breeding data to get annual stats.\n",
      "Tue Apr  4 20:55:45 2017 - Writing intermediate table df_clutch_count to disk.\n",
      "Tue Apr  4 20:55:45 2017 - Written ./output/A_load_and_combine_data/df_clutch_count.csv: 0.015 MB\n",
      "Tue Apr  4 20:55:45 2017 - Writing intermediate table df_breeding_gb to disk.\n",
      "Tue Apr  4 20:55:45 2017 - Written ./output/A_load_and_combine_data/df_breeding_gb.csv: 0.015 MB\n"
     ]
    }
   ],
   "source": [
    "log('Aggregating breeding data to get annual stats.')\n",
    "\n",
    "# get the clutches per nest and year\n",
    "# [[chosen columns]] -> groupby -> apply max -> add suffix -> remove multi-index\n",
    "df_clutch_count = df_breeding[['nest_id', 'breeding_year', 'clutch']\n",
    "                             ].groupby(['nest_id', 'breeding_year']).max().add_suffix('_count').reset_index()\n",
    "\n",
    "# get the annual stats per nest, year and clutch\n",
    "temp = df_breeding[['nest_id', 'breeding_year', 'ActivityStatus', 'clutch', 'eggs', 'ChicksDead', 'TotalChicksHatch',\n",
    "                    'ChicksFledge', 'EggLayDate', 'ChicksAge', 'MassChick1', 'MassChick2'\n",
    "                   ]].copy()\n",
    "aggregation = {\n",
    "    'eggs': {'egg_count': 'max'},\n",
    "    'ChicksDead': {'num_deaths': 'sum'}, \n",
    "    'TotalChicksHatch': {'num_hatched': 'max'},\n",
    "    'ChicksFledge': {'num_fledged': 'max'}, \n",
    "    'EggLayDate': {'EggLayDate': 'max'}, \n",
    "    'ChicksAge': {'age_at_fledge': 'max'},\n",
    "    'MassChick1': {'peak_mass_chick1': 'max'},\n",
    "    'MassChick2': {'peak_mass_chick2': 'max'}    \n",
    "}\n",
    "df_breeding_gb = temp.groupby(['nest_id', 'breeding_year', 'clutch']).agg(aggregation).reset_index()\n",
    "# df_breeding_gb is now a multi-index, which is a pain. Lose the top level with droplevel(), which loses the first 3 column names\n",
    "# Get them back by explicitly renaming.\n",
    "df_breeding_gb.columns = df_breeding_gb.columns.droplevel(0)\n",
    "df_breeding_gb.columns.values[0] = 'nest_id'\n",
    "df_breeding_gb.columns.values[1] = 'breeding_year'\n",
    "df_breeding_gb.columns.values[2] = 'clutch'\n",
    "df_breeding_gb['EggLayDate'] = df_breeding_gb['EggLayDate'].apply(lambda x: pd.to_datetime(x)) # confirmed the max EggLayDate is correct when converted back to datetime\n",
    "\n",
    "write_temp_file(df_clutch_count, './output/A_load_and_combine_data/df_clutch_count.csv', 'df_clutch_count')\n",
    "write_temp_file(df_breeding_gb, './output/A_load_and_combine_data/df_breeding_gb.csv', 'df_breeding_gb')\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Load the combined temperature and humidity data\n",
    "Read the temperature and humidity data file into memory and report on success/failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:55:45 2017 - Loading the Temperature and Humidity data file.\n",
      "Tue Apr  4 20:55:45 2017 - Temperature and Humidity file is 112.663 MB.\n",
      "Tue Apr  4 20:55:45 2017 - Loading into memory. Please be patient.\n",
      "Tue Apr  4 20:56:02 2017 - Success: loaded 2,505,456 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(temp_humid_file, 'Temperature and Humidity')\n",
    "column_names = ['recnum', 'datetime', 'nest_id', 'humidity', 'temp_c']\n",
    "data_types = {'recnum': np.int32, \n",
    "              'datetime': str, \n",
    "              'nest_id': str,\n",
    "              'humidity': np.float32,\n",
    "              'temp_c': np.float32\n",
    "             }\n",
    "df_sensor_data = pd.read_csv(temp_humid_file,\n",
    "                      names=column_names,\n",
    "                      usecols=[0,1,2,3,4],\n",
    "                      dtype=data_types,\n",
    "#                           nrows=10000,\n",
    "                      parse_dates=['datetime'],\n",
    "                      infer_datetime_format=True,\n",
    "                      dayfirst=True,\n",
    "                      encoding='utf-8',\n",
    "                      error_bad_lines=True,\n",
    "                      warn_bad_lines=True\n",
    "                     )\n",
    "read_file_handler_end(temp_humid_file, 'Temperature and Humidity', df_sensor_data, 'df_sensor_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Calculations per-sensor reading\n",
    "The following calculations are added per sensor reading:\n",
    "* The `breeding_year`: same as the calendar year\n",
    "* `temp_bucket` is a category for each 5C temperature range: <0, 0-5, .., 60+\n",
    "* `humidity_bucket`: is a category for roughly 20% humidity ranges, based on human comfort zones\n",
    "* `average_activity_phase`: the average activity conducted at the time of the observation\n",
    "\n",
    "Note: An `actual_activity_phase` (the current phase of breeding based on per-nest observations) is added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:02 2017 - Cleanse the nest_ids...\n",
      "Tue Apr  4 20:56:04 2017 -  Done.\n",
      "Tue Apr  4 20:56:04 2017 - Calculating breeding year. Be patient...\n",
      "Tue Apr  4 20:56:16 2017 -  Done.\n",
      "Tue Apr  4 20:56:16 2017 - Calculating temperature buckets. Be patient...\n",
      "Tue Apr  4 20:56:21 2017 -  Done.\n",
      "Tue Apr  4 20:56:21 2017 - Calculating humidity buckets. Be patient...\n",
      "Tue Apr  4 20:56:28 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# make sure the nest IDs are all caps\n",
    "log('Cleanse the nest_ids...')\n",
    "df_sensor_data['nest_id'] = df_sensor_data['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "log(' Done.')\n",
    "\n",
    "# add the breeding_year (same as calendar year): \n",
    "log('Calculating breeding year. Be patient...')\n",
    "df_sensor_data['breeding_year'] = df_sensor_data['datetime'].apply(lambda x: breeding_year(x))\n",
    "log(' Done.')\n",
    "\n",
    "# Add flags for various temperature ranges. Uses temp_bucket() as defined in helper_functions.py\n",
    "# These are summed to give the amount of time in the temp band\n",
    "log('Calculating temperature buckets. Be patient...')\n",
    "df_sensor_data['temp_bucket'] = df_sensor_data['temp_c'].apply(temp_bucket)\n",
    "log(' Done.')\n",
    "\n",
    "# # Add flags for various humidity ranges. Uses humidity_bucket() as defined in helper_functions.py\n",
    "# # These are summed to give the amount of time in the humidity band\n",
    "log('Calculating humidity buckets. Be patient...')\n",
    "df_sensor_data['humidity_bucket'] = df_sensor_data['humidity'].apply(humidity_bucket)\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Join the NestCharacteristic Static and Seasonal data\n",
    "`df_nest_seasonal` + `df_nest_static` -> `df_nest_joined`\n",
    "\n",
    "Seasonal is `left` and Static is `right`, such that the Seasonal data is augmented with the nests static metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:28 2017 - Joining the Nest Characteristic (Seasonal and Static) data sets .\n",
      "Tue Apr  4 20:56:28 2017 - Join complete. Here are the stats:\n",
      "Records in seasonal data:                   1,929\n",
      "Records in static data:                       247\n",
      "                              -------------------\n",
      "Records in joined data:                     1,929\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids:                           241\n",
      "\n",
      "Tue Apr  4 20:56:28 2017 - Writing intermediate table df_nest_joined to disk.\n",
      "Tue Apr  4 20:56:28 2017 - Written ./output/A_load_and_combine_data/df_nest_joined.csv: 0.526 MB\n",
      "Tue Apr  4 20:56:28 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "log('Joining the Nest Characteristic (Seasonal and Static) data sets .')\n",
    "df_nest_joined = pd.merge(left=df_nest_seasonal,\n",
    "                            right=df_nest_static,\n",
    "                            how='left',\n",
    "                            on=['nest_id'], # both have same keys\n",
    "                            left_on=None, # same key names: don't need to specify R and L\n",
    "                            right_on=None, # same key names: don't need to specify R and L\n",
    "                            left_index=False, # dont' use left df index as key\n",
    "                            right_index=False, # dont' use right df index as key\n",
    "                            sort=True, # for efficiency do/not sort the df first\n",
    "                            suffixes=['_seasonal', '_static']\n",
    "                            )\n",
    "if df_nest_joined is not None:\n",
    "    log('Join complete. Here are the stats:')\n",
    "    print('Records in seasonal data:    {0:>20,}'.format(len(df_nest_seasonal)))\n",
    "    print('Records in static data:      {0:>20,}'.format(len(df_nest_static)))\n",
    "    print('                              -------------------')\n",
    "    print('Records in joined data:      {0:>20,}'.format(len(df_nest_joined)))\n",
    "    print('\\nOverview:')\n",
    "    gb = df_nest_joined.groupby(['nest_id'])\n",
    "    print('Number of nest_ids:          {0:>20,}\\n'.format(len(gb)))\n",
    "    write_temp_file(df_nest_joined, './output/A_load_and_combine_data/df_nest_joined.csv', 'df_nest_joined')\n",
    "    del gb\n",
    "else:\n",
    "    log('JOIN FAILED!!!.')\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Join the breeding stats together\n",
    "`df_breeding_gb + df_clutch_count -> df_breeding_annual_stats`\n",
    "\n",
    "Clutch counts per year and annual clutch survival stats.\n",
    "Note that the reduced record count in `df_breeding_annual_stats` compared to `df_clutch_count` is due to a number of nest-years having blank/zero clutches. These are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:28 2017 - Merging the aggregated breeding stats.\n",
      "Tue Apr  4 20:56:28 2017 - Join complete. Here are the stats:\n",
      "Records in annual stats data:                  277\n",
      "Records in clutch count data:                  860\n",
      "                              -------------------\n",
      "Records in joined data:                        277\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids in clutch count:            212\n",
      "Number of nest_ids in breeding stats:          122\n",
      "Number of nest_ids in joined:                  122\n",
      "\n",
      "Tue Apr  4 20:56:28 2017 - Writing intermediate table df_breeding_annual_stats to disk.\n",
      "Tue Apr  4 20:56:28 2017 - Written ./output/A_load_and_combine_data/df_breeding_annual_stats.csv: 0.016 MB\n",
      "Tue Apr  4 20:56:28 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# join the clutch count on to the annual stats\n",
    "log('Merging the aggregated breeding stats.')\n",
    "df_breeding_annual_stats = pd.merge(left=df_breeding_gb,\n",
    "                                    right=df_clutch_count,\n",
    "                                    how='left',\n",
    "                                    on=['nest_id', 'breeding_year'], # both have same keys\n",
    "                                    sort=True # for efficiency do/not sort the df first\n",
    "                                   )\n",
    "\n",
    "log('Join complete. Here are the stats:')\n",
    "print('Records in annual stats data: {0:>20,}'.format(len(df_breeding_gb)))\n",
    "print('Records in clutch count data: {0:>20,}'.format(len(df_clutch_count)))\n",
    "print('                              -------------------')\n",
    "print('Records in joined data:       {0:>20,}'.format(len(df_breeding_annual_stats)))\n",
    "print('\\nOverview:')\n",
    "gb = df_breeding_annual_stats.groupby(['nest_id'])\n",
    "print('Number of nest_ids in clutch count:   {0:>12,}'.format(len(df_clutch_count.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in breeding stats: {0:>12,}'.format(len(df_breeding_gb.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in joined:         {0:>12,}\\n'.format(len(df_breeding_annual_stats.groupby(['nest_id']))))\n",
    "write_temp_file(df_breeding_annual_stats, './output/A_load_and_combine_data/df_breeding_annual_stats.csv', 'df_breeding_annual_stats')\n",
    "del gb\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Join the Nest data (seasonal and static) to the Breeding stats\n",
    "`df_nest_joined + df_breeding_annual_stats -> df_nest_and_breeding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:28 2017 - Merging the aggregated breeding stats to the static and seasonal nest data.\n",
      "Tue Apr  4 20:56:28 2017 - Join complete. Here are the stats:\n",
      "Records in nest data:                        1,929\n",
      "Records in breeding stats data:                277\n",
      "                                      ------------\n",
      "Records in joined data:                      2,066\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids in nest data:               241\n",
      "Number of nest_ids in breeding stats:          122\n",
      "Number of nest_ids in joined:                  241\n",
      "\n",
      "Tue Apr  4 20:56:28 2017 - Writing intermediate table df_nest_and_breeding to disk.\n",
      "Tue Apr  4 20:56:28 2017 - Written ./output/A_load_and_combine_data/NestDataWithBreedingStats.csv: 0.604 MB\n",
      "Tue Apr  4 20:56:28 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "# join the annual clutch and breeding stats onto the full seasonal and static nest data\n",
    "log('Merging the aggregated breeding stats to the static and seasonal nest data.')\n",
    "df_nest_and_breeding = pd.merge(left=df_nest_joined,\n",
    "                                right=df_breeding_annual_stats,\n",
    "                                how='left',\n",
    "                                on=['nest_id','breeding_year'], # both have same keys\n",
    "                                sort=True \n",
    "                               )\n",
    "\n",
    "log('Join complete. Here are the stats:')\n",
    "print('Records in nest data:                 {0:>12,}'.format(len(df_nest_joined)))\n",
    "print('Records in breeding stats data:       {0:>12,}'.format(len(df_breeding_annual_stats)))\n",
    "print('                                      ------------')\n",
    "print('Records in joined data:               {0:>12,}'.format(len(df_nest_and_breeding)))\n",
    "print('\\nOverview:')\n",
    "gb = df_breeding_annual_stats.groupby(['nest_id'])\n",
    "print('Number of nest_ids in nest data:      {0:>12,}'.format(len(df_nest_joined.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in breeding stats: {0:>12,}'.format(len(df_breeding_annual_stats.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in joined:         {0:>12,}\\n'.format(len(df_nest_and_breeding.groupby(['nest_id']))))\n",
    "write_temp_file(df_nest_and_breeding, './output/A_load_and_combine_data/NestDataWithBreedingStats.csv', 'df_nest_and_breeding')\n",
    "del gb\n",
    "log( 'Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Aggregate the sensor data into stats per breeding phase\n",
    "To understand the effect of nest conditions (from sensor data) in the choice of nest and breeding success of the nest, we need to break up the stats into:\n",
    "* *annual stats* which represent the averages, spikes etc for the entire year. These give an understanding of the nest itself.\n",
    "* *phase stats* which represent the conditions during specific phases of the breeding cycle. E.g. during nesting, during incubation, during rearing. To get these phase stats, we need to get the phase boundary dates from the breeding observation data.\n",
    "\n",
    "The nest sensor readings are aggregated to summarise the nest conditions by `nest`, `breeding_year` and `activity_phase`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5.1 Get the actual breeding phase dates\n",
    "Summarise the breeding data to obtain the following:\n",
    "* list of all nests (regardless of breeding activity)\n",
    "* the `nesting_date` for each nest in each year\n",
    "* the `egg_lay_date` for each nest, year and clutch\n",
    "* the `hatch_date` for each nest, year and clutch\n",
    "* the `fledge_date` for each nest, year and clutch\n",
    "\n",
    "Join these all back together to get the phase dates all in one place, then join the combined result on to the sensor data table and calculate the phase in which each sensor reading occurred.\n",
    "This will take a while.\n",
    "\n",
    "**Issue: Nesting dates dont work: the second clutch will have first nesting date and the first obs for many nests is after the lay date, so nesting_date > lay_date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:28 2017 - Calculating the breeding phase dates for each nest and year.\n",
      "Tue Apr  4 20:56:28 2017 - Writing intermediate table df_all_nests to disk.\n",
      "Tue Apr  4 20:56:28 2017 - Written ./output/A_load_and_combine_data/df_all_nests.csv: 0.002 MB\n",
      "Tue Apr  4 20:56:28 2017 -  Creating a table for the courting and egg lay dates...\n",
      "Tue Apr  4 20:56:28 2017 - Writing intermediate table gb_lay_date to disk.\n",
      "Tue Apr  4 20:56:28 2017 - Written ./output/A_load_and_combine_data/gb_lay_date.csv: 0.011 MB\n",
      "Tue Apr  4 20:56:28 2017 -  Done.\n",
      "Tue Apr  4 20:56:28 2017 -  Creating a table for the hatch dates...\n",
      "Tue Apr  4 20:56:29 2017 - Writing intermediate table gb_hatch_date to disk.\n",
      "Tue Apr  4 20:56:29 2017 - Written ./output/A_load_and_combine_data/gb_hatch_date.csv: 0.007 MB\n",
      "Tue Apr  4 20:56:29 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# for each nest, year and clutch, get the following:\n",
    "# first activity_status date (nesting_date), EggLayDate, hatch_date, fledge_date\n",
    "# nesting_date, hatch_date, fledge_date are the min observation_date per nest, year, clutch where the value is not NaN\n",
    "\n",
    "log('Calculating the breeding phase dates for each nest and year.')\n",
    "# take each unique nest id, which we'll join the rest of the data onto\n",
    "df_all_nests = df_nest_static[['nest_id']].drop_duplicates()\n",
    "write_temp_file(df_all_nests, './output/A_load_and_combine_data/df_all_nests.csv', 'df_all_nests')\n",
    "\n",
    "# egg_lay_date\n",
    "log(' Creating a table for the courting and egg lay dates...')\n",
    "gb_lay_date = df_breeding[['nest_id', 'breeding_year', 'clutch', 'EggLayDate']\n",
    "                         ].groupby(['nest_id', 'breeding_year', 'clutch']).min().reset_index()\n",
    "gb_lay_date.rename(columns={'EggLayDate': 'egg_lay_date'}, inplace=True)\n",
    "# nesting date: 31 days before egg_lay_date\n",
    "gb_lay_date['courting_date'] = gb_lay_date['egg_lay_date'] - datetime.timedelta(days=31)\n",
    "write_temp_file(gb_lay_date, './output/A_load_and_combine_data/gb_lay_date.csv', 'gb_lay_date')\n",
    "log(' Done.')\n",
    "\n",
    "# hatch_date\n",
    "log(' Creating a table for the hatch dates...')\n",
    "def hatch_date(row):\n",
    "    return row['observation_date'] - datetime.timedelta(days=row['ChicksAge'])\n",
    "# get the observation date (select columns) where age is not blank (i.e. they're there)\n",
    "gb_hatch_date = df_breeding[['nest_id', 'breeding_year', 'clutch', \n",
    "                             'observation_date', 'ChicksAge']].loc[df_breeding['ChicksAge'].notnull()]\n",
    "gb_hatch_date['hatch_date'] = gb_hatch_date.apply(hatch_date, axis=1)\n",
    "# get the min hatch_date \n",
    "gb_hatch_date = gb_hatch_date[['nest_id', 'breeding_year', 'clutch', 'hatch_date'\n",
    "                              ]].groupby(['nest_id', 'breeding_year', 'clutch']).min().reset_index()\n",
    "write_temp_file(gb_hatch_date, './output/A_load_and_combine_data/gb_hatch_date.csv', 'gb_hatch_date')\n",
    "log(' Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:29 2017 -  Creating a table for the fledge dates...\n",
      "Tue Apr  4 20:56:31 2017 - Writing intermediate table gb_fledge_date to disk.\n",
      "Tue Apr  4 20:56:31 2017 - Written ./output/A_load_and_combine_data/gb_fledge_date.csv: 0.007 MB\n",
      "Tue Apr  4 20:56:31 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# fledge_date\n",
    "log(' Creating a table for the fledge dates...')\n",
    "# is either the date that the chicks were of age and no longer observed in the nest, or were observed dead\n",
    "# get the observation date (select columns) where there is a fledge flag\n",
    "gb_fledge_date = df_breeding[['nest_id', 'breeding_year', 'clutch', 'observation_date', \n",
    "                              'ChicksAlive', 'ChicksDead', 'ChicksFledge']].fillna(0)\n",
    "gb_fledge_date['dead_or_fledged'] = gb_fledge_date.apply(lambda row: row['ChicksFledge'] > 0 \n",
    "                                                         or (row['ChicksDead'] > 0 and row['ChicksAlive'] == 0), axis=1)\n",
    "gb_fledge_date = gb_fledge_date.query('dead_or_fledged')\n",
    "# get the min obs date, which is the earliest fledge recording (per clutch)\n",
    "gb_fledge_date = gb_fledge_date[['nest_id', 'breeding_year', 'clutch', 'observation_date'\n",
    "                                ]].groupby(['nest_id', 'breeding_year', 'clutch']).min().reset_index()\n",
    "# rename the obs date \n",
    "gb_fledge_date.rename(columns={'observation_date': 'dead_or_fledge_date'}, inplace=True)\n",
    "write_temp_file(gb_fledge_date, './output/A_load_and_combine_data/gb_fledge_date.csv', 'gb_fledge_date')\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_phase_dates` contains the full breeding data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(df_breeding.breeding_year[0].dtype)\n",
    "print(gb_lay_date.breeding_year[0].dtype)\n",
    "print(gb_hatch_date.breeding_year[0].dtype)\n",
    "print(gb_fledge_date.breeding_year[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:31 2017 -  Merging the phase date tables...\n",
      "Tue Apr  4 20:56:31 2017 - Writing intermediate table df_phase_dates to disk.\n",
      "Tue Apr  4 20:56:31 2017 - Written ./output/A_load_and_combine_data/df_phase_dates.csv: 0.018 MB\n",
      "Tue Apr  4 20:56:31 2017 -  Done.\n"
     ]
    }
   ],
   "source": [
    "# join the key date tables together\n",
    "log(' Merging the phase date tables...')\n",
    "df_phase_dates = pd.merge(left=df_all_nests, \n",
    "                          right=gb_lay_date, \n",
    "                          how='left', \n",
    "                          on=['nest_id'], \n",
    "                          sort=True\n",
    "                         )\n",
    "df_phase_dates = pd.merge(left=df_phase_dates, \n",
    "                          right=gb_hatch_date, \n",
    "                          how='left', \n",
    "                          on=['nest_id', 'breeding_year', 'clutch'], \n",
    "                          sort=True\n",
    "                         )\n",
    "df_phase_dates = pd.merge(left=df_phase_dates, \n",
    "                          right=gb_fledge_date, \n",
    "                          how='left', \n",
    "                          on=['nest_id', 'breeding_year', 'clutch'], \n",
    "                          sort=True\n",
    "                         )\n",
    "write_temp_file(df_phase_dates, './output/A_load_and_combine_data/df_phase_dates.csv', 'df_phase_dates')\n",
    "log(' Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:31 2017 -  Pivot breeding data to get the clutch dates...\n",
      "Tue Apr  4 20:56:31 2017 - Writing intermediate table df_clutch_pivot to disk.\n",
      "Tue Apr  4 20:56:31 2017 - Written ./output/A_load_and_combine_data/df_clutch_pivot.csv: 0.007 MB\n",
      "Tue Apr  4 20:56:31 2017 -  Done.\n",
      "Tue Apr  4 20:56:31 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "log(' Pivot breeding data to get the clutch dates...')\n",
    "# get the required cols\n",
    "df_clutch_pivot = gb_lay_date[['nest_id', 'breeding_year', 'clutch', 'egg_lay_date']].copy()\n",
    "# we have to combine the index (because pivot() does not like a multi-index) and then split it apart a few steps later\n",
    "df_clutch_pivot['nestyear'] = df_clutch_pivot['nest_id'] + '-' + (df_clutch_pivot['breeding_year'].apply(lambda x: str(int(x))))\n",
    "# drop the old index fields\n",
    "df_clutch_pivot = df_clutch_pivot[['nestyear', 'clutch', 'egg_lay_date']]\n",
    "# do the pivot to get the (up to three) clutch dates per nest and year\n",
    "df_clutch_pivot = df_clutch_pivot.pivot(index='nestyear', columns='clutch')['egg_lay_date'].reset_index()\n",
    "\n",
    "# rename and restore the indexes\n",
    "df_clutch_pivot.rename(columns={1.0: 'clutch_1', 2.0: 'clutch_2', 3.0: 'clutch_3'}, inplace=True)\n",
    "df_clutch_pivot['nest_id'] = df_clutch_pivot['nestyear'].apply(lambda x: x.split('-')[0])\n",
    "df_clutch_pivot['breeding_year'] = df_clutch_pivot['nestyear'].apply(lambda x: float(x.split('-')[1]))\n",
    "df_clutch_pivot = df_clutch_pivot[['nest_id', 'breeding_year', 'clutch_1', 'clutch_2', 'clutch_3']]\n",
    "write_temp_file(df_clutch_pivot, './output/A_load_and_combine_data/df_clutch_pivot.csv', 'df_clutch_pivot')\n",
    "log(' Done.')\n",
    "log('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add clutch dates to the sensor data** \n",
    "\n",
    "The `sensor_data` are lacking a `clutch` number, which will create duplicates if we attempt to join on the phase dates. Get the clutch dates and join them into the `sensor_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(df_breeding.breeding_year[0].dtype)\n",
    "print(gb_lay_date.breeding_year[0].dtype)\n",
    "print(gb_hatch_date.breeding_year[0].dtype)\n",
    "print(gb_fledge_date.breeding_year[0].dtype)\n",
    "print(df_clutch_pivot.breeding_year[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 20:56:31 2017 - Join the clutch dates to the sensor data.\n",
      "Tue Apr  4 20:56:34 2017 - Done. Rows: 2,505,456\n",
      "Tue Apr  4 20:56:34 2017 - Assigning a clutch number to each sensor record. Be patient.\n",
      "Tue Apr  4 21:02:33 2017 - Done.\n",
      "Tue Apr  4 21:02:33 2017 - Writing intermediate table df_sensor_clutch to disk.\n",
      "Tue Apr  4 21:03:31 2017 - Written ./output/A_load_and_combine_data/df_sensor_clutch.csv: 287.631 MB\n"
     ]
    }
   ],
   "source": [
    "# to avoid making epic dupes, we need to first add the clutch number on to the sensor data table\n",
    "\n",
    "log('Join the clutch dates to the sensor data.)\n",
    "# join on to the sensor data\n",
    "df_sensor_clutch = pd.merge(left=df_sensor_data, \n",
    "                            right=df_clutch_pivot, \n",
    "                            how='left', \n",
    "                            on=['nest_id', 'breeding_year'],\n",
    "                            sort=True\n",
    "                           )\n",
    "log('Done. Rows: {0:,}'.format(len(df_sensor_clutch)))\n",
    "\n",
    "log('Assigning a clutch number to each sensor record. Be patient.')\n",
    "# flag each reading with a clutch number\n",
    "def clutch_number(row):\n",
    "    if pd.isnull(row['clutch_1']):\n",
    "        # there are no breeding observations for this nest and year\n",
    "        return 0\n",
    "    else:\n",
    "        # there is at least 1 clutch\n",
    "        if pd.isnull(row['clutch_2']) or row['datetime'] < row['clutch_2']:\n",
    "            # there was only a single clutch, or there were >1 but this reading was before the 2nd clutch\n",
    "            return 1\n",
    "        elif pd.isnull(row['clutch_3']) or (not pd.isnull(row['clutch_3']) and row['datetime'] < row['clutch_3']):\n",
    "            # there is a 2nd clutch if we got this far. if there is no 3rd, or the reading is before the 3rd, then this is 2nd\n",
    "            return 2\n",
    "        else:\n",
    "            # there is a 3rd clutch and the sensor reading is after the 3rd\n",
    "            return 3\n",
    "\n",
    "df_sensor_clutch['clutch_number'] = df_sensor_clutch.apply(lambda row: clutch_number(row), axis=1)\n",
    "log('Done.')\n",
    "write_temp_file(df_sensor_clutch, './output/A_load_and_combine_data/df_sensor_clutch.csv', 'df_sensor_clutch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the actual breeding phase dates on to the sensor data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 21:03:31 2017 - Join the phase dates on to the sensor data.\n",
      "Tue Apr  4 21:03:34 2017 - Done. Rows: 2,505,456\n"
     ]
    }
   ],
   "source": [
    "log('Join the phase dates on to the sensor data.')\n",
    "df_sensor_phase = pd.merge(left=df_sensor_clutch,\n",
    "                        right=df_phase_dates,\n",
    "                        how='left',\n",
    "                        left_on=['nest_id', 'breeding_year', 'clutch_number'], # same key names: don't need to specify R and L\n",
    "                        right_on=['nest_id', 'breeding_year', 'clutch'], # same key names: don't need to specify R and L\n",
    "                        sort=True # for efficiency do/not sort the df first\n",
    "#                             suffixes=['_temp', '_humd']\n",
    "                        )\n",
    "log('Done. Rows: {0:,}'.format(len(df_sensor_phase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the aggregate temperature and humidity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Calculating the aggregate sensor stats...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: **Number of days per month with a temp >= 35C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  5 08:53:56 2017 -    1. Days per month >= 35C...\n",
      "Wed Apr  5 08:53:56 2017 -    1. Done\n"
     ]
    }
   ],
   "source": [
    "log('   1. Days per month >= 35C...')\n",
    "# get the records > 35\n",
    "# Convert the datetime to a month and day (in addition to the existing breeding_year)\n",
    "# Count the distinct dates per nest per year per month\n",
    "df_temp_above_35C = df_sensor_phase.loc[df_sensor_phase['temp_c'] >= 35].reset_index()\n",
    "df_temp_above_35C['month'] = df_temp_above_35C['datetime'].apply(lambda x: x.month)\n",
    "df_temp_above_35C['day'] = df_temp_above_35C['datetime'].apply(lambda x: x.day)\n",
    "gb_monthly_days_above_35C = df_temp_above_35C.groupby(['nest_id', 'breeding_year', 'month']).size()\n",
    "log('   1. Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: **Number of days per season with a temp >= 35C**\n",
    "\n",
    "**Note**: Assumes that Summer 2013 is Jan-Feb13 and Dec13; i.e. all the summer months in the year 2013 rather than the Summer season that starts in 2013 (which would be Dec13-Feb14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  5 09:23:15 2017 -    2. Days per season >= 35C...\n",
      "Wed Apr  5 09:23:15 2017 -    2. Done\n"
     ]
    }
   ],
   "source": [
    "log('   2. Days per season >= 35C...')\n",
    "# use the >35C table from #1, sum for each season\n",
    "def season_from_month(month):\n",
    "    '''\n",
    "    Returns the season (southern hemisphere) for the provided month.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    month (scalar): the float/integer representing the calendar month\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A string representing the season (southern hemisphere) for the calendar month provided. \n",
    "    I.e. 'Spring', 'Summer', 'Autumn', 'Winter'\n",
    "    '''\n",
    "    if 3 <= month <= 5:\n",
    "        return 'Autumn'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'Winter'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'Spring'\n",
    "    elif month == 12 or 1 <= month <= 2:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        raise ValueError('{0} is not a valid month. Expecting 1-12.'.format(month))\n",
    "\n",
    "df_temp_above_35C['season'] = df_temp_above_35C['month'].apply(lambda x: season_from_month(x))\n",
    "gb_seasonal_days_above_35C = df_temp_above_35C.groupby(['nest_id', 'breeding_year', 'season']).size()\n",
    "log('   2. Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: **Number of days per year with a temp >= 35C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  5 09:25:24 2017 -    3. Days per year >= 35C...\n",
      "Wed Apr  5 09:25:24 2017 -    3. Done\n"
     ]
    }
   ],
   "source": [
    "log('   3. Days per year >= 35C...')\n",
    "# use the >35C table from #1, sum for each year\n",
    "gb_annual_days_above_35C = df_temp_above_35C.groupby(['nest_id', 'breeding_year']).size()\n",
    "log('   3. Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: **Number of days per month with a temp >= 40C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  5 09:27:05 2017 -    4. Days per month >= 40C...\n",
      "Wed Apr  5 09:27:05 2017 -    4. Done\n"
     ]
    }
   ],
   "source": [
    "log('   4. Days per month >= 40C...')\n",
    "# get the records > 40\n",
    "# Convert the datetime to a month and day (in addition to the existing breeding_year)\n",
    "# Count the distinct dates per nest per year per month\n",
    "df_temp_above_40C = df_sensor_phase.loc[df_sensor_phase['temp_c'] >= 40].reset_index()\n",
    "df_temp_above_40C['month'] = df_temp_above_40C['datetime'].apply(lambda x: x.month)\n",
    "df_temp_above_40C['day'] = df_temp_above_40C['datetime'].apply(lambda x: x.day)\n",
    "gb_monthly_days_above_40C = df_temp_above_40C.groupby(['nest_id', 'breeding_year', 'month']).size()\n",
    "log('   4. Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: **Number of days per season with a temp >= 40C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  5 09:28:41 2017 -    5. Days per season >= 40C...\n",
      "Wed Apr  5 09:28:41 2017 -    5. Done\n"
     ]
    }
   ],
   "source": [
    "log('   5. Days per season >= 40C...')\n",
    "# use the >40C table from #4, sum for each season\n",
    "df_temp_above_40C['season'] = df_temp_above_40C['month'].apply(lambda x: season_from_month(x))\n",
    "gb_seasonal_days_above_40C = df_temp_above_40C.groupby(['nest_id', 'breeding_year', 'season']).size()\n",
    "log('   5. Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: **Number of days per year with a temp >= 40C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  5 09:29:26 2017 -    6. Days per year >= 40C...\n",
      "Wed Apr  5 09:29:26 2017 -    6. Done\n"
     ]
    }
   ],
   "source": [
    "log('   6. Days per year >= 40C...')\n",
    "# use the >35C table from #1, sum for each year\n",
    "gb_annual_days_above_40C = df_temp_above_40C.groupby(['nest_id', 'breeding_year']).size()\n",
    "log('   6. Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation: ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Calculating the sensor stats: Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  5 08:27:06 2017 - Calculating the sensor stats...\n",
      "Wed Apr  5 08:27:06 2017 - 1 - Calculating the sensor stats...\n",
      "Wed Apr  5 08:27:07 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# mean daily/ monthly/ seasonal Temp/RH\n",
    "\n",
    "# mean daily/ monthly / seasonal min Temp/RH\n",
    "\n",
    "# mean daily/ monthly/ seasonal Max Temp/RH\n",
    "\n",
    "# mean daily /monthly/ seasonal Range Temp/RH\n",
    "\n",
    "# No. hours exceeding 35C+ 40C\n",
    "\n",
    "# No. hours exceeding 35C + 40C /month/season\n",
    "\n",
    "# mean Temp/RH during incubation/chick rearing\n",
    "\n",
    "# mean daily max/RH during incubation/chick rearing\n",
    "\n",
    "# mean daily min/RH during incubation/chick rearing\n",
    "\n",
    "# mean daily range /RH during incubation/chick rearing\n",
    "\n",
    "# deviation from ambient temp/RH\n",
    "\n",
    "# mean hourly temp + RH by daily/ monthly/ seasonally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nest_id  breeding_year\n",
       "101      2013               3\n",
       "         2014               6\n",
       "102      2013             179\n",
       "         2014             107\n",
       "103      2015              63\n",
       "         2016              43\n",
       "108      2013             134\n",
       "         2014             138\n",
       "         2015              80\n",
       "         2016              21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_days_above_40C.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recnum</th>\n",
       "      <th>datetime</th>\n",
       "      <th>nest_id</th>\n",
       "      <th>humidity</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>breeding_year</th>\n",
       "      <th>temp_bucket</th>\n",
       "      <th>humidity_bucket</th>\n",
       "      <th>clutch_1</th>\n",
       "      <th>clutch_2</th>\n",
       "      <th>clutch_3</th>\n",
       "      <th>clutch_number</th>\n",
       "      <th>clutch</th>\n",
       "      <th>egg_lay_date</th>\n",
       "      <th>courting_date</th>\n",
       "      <th>hatch_date</th>\n",
       "      <th>dead_or_fledge_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200512</td>\n",
       "      <td>2013-07-11 21:49:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.940002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200540</td>\n",
       "      <td>2013-07-11 22:04:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.940002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200570</td>\n",
       "      <td>2013-07-11 22:19:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.510002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200602</td>\n",
       "      <td>2013-07-11 22:34:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.510002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200638</td>\n",
       "      <td>2013-07-11 22:49:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.510002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200674</td>\n",
       "      <td>2013-07-11 23:04:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200710</td>\n",
       "      <td>2013-07-11 23:19:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200748</td>\n",
       "      <td>2013-07-11 23:34:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200788</td>\n",
       "      <td>2013-07-11 23:49:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>275432</td>\n",
       "      <td>2013-07-12 00:04:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>275475</td>\n",
       "      <td>2013-07-12 00:19:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>275518</td>\n",
       "      <td>2013-07-12 00:34:00</td>\n",
       "      <td>101</td>\n",
       "      <td>90.199997</td>\n",
       "      <td>16.110001</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>275561</td>\n",
       "      <td>2013-07-12 00:49:00</td>\n",
       "      <td>101</td>\n",
       "      <td>90.199997</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>275604</td>\n",
       "      <td>2013-07-12 01:04:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>275647</td>\n",
       "      <td>2013-07-12 01:19:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.510002</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>275690</td>\n",
       "      <td>2013-07-12 01:34:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.940002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>275733</td>\n",
       "      <td>2013-07-12 01:49:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.940002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>277496</td>\n",
       "      <td>2013-07-12 02:04:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.940002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>277539</td>\n",
       "      <td>2013-07-12 02:19:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>277582</td>\n",
       "      <td>2013-07-12 02:34:00</td>\n",
       "      <td>101</td>\n",
       "      <td>91.510002</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2013</td>\n",
       "      <td>temp_10-20</td>\n",
       "      <td>RH%_80-100</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    recnum            datetime nest_id   humidity     temp_c  breeding_year  \\\n",
       "0   200512 2013-07-11 21:49:00     101  91.940002  15.100000           2013   \n",
       "1   200540 2013-07-11 22:04:00     101  91.940002  15.100000           2013   \n",
       "2   200570 2013-07-11 22:19:00     101  91.510002  15.600000           2013   \n",
       "3   200602 2013-07-11 22:34:00     101  91.510002  15.600000           2013   \n",
       "4   200638 2013-07-11 22:49:00     101  91.510002  15.600000           2013   \n",
       "5   200674 2013-07-11 23:04:00     101  91.080002  15.600000           2013   \n",
       "6   200710 2013-07-11 23:19:00     101  91.080002  15.600000           2013   \n",
       "7   200748 2013-07-11 23:34:00     101  91.080002  15.600000           2013   \n",
       "8   200788 2013-07-11 23:49:00     101  91.080002  15.600000           2013   \n",
       "9   275432 2013-07-12 00:04:00     101  91.080002  15.100000           2013   \n",
       "10  275475 2013-07-12 00:19:00     101  91.080002  15.600000           2013   \n",
       "11  275518 2013-07-12 00:34:00     101  90.199997  16.110001           2013   \n",
       "12  275561 2013-07-12 00:49:00     101  90.199997  15.600000           2013   \n",
       "13  275604 2013-07-12 01:04:00     101  91.080002  15.600000           2013   \n",
       "14  275647 2013-07-12 01:19:00     101  91.510002  15.600000           2013   \n",
       "15  275690 2013-07-12 01:34:00     101  91.940002  15.100000           2013   \n",
       "16  275733 2013-07-12 01:49:00     101  91.940002  15.100000           2013   \n",
       "17  277496 2013-07-12 02:04:00     101  91.940002  15.100000           2013   \n",
       "18  277539 2013-07-12 02:19:00     101  91.080002  15.100000           2013   \n",
       "19  277582 2013-07-12 02:34:00     101  91.510002  15.100000           2013   \n",
       "\n",
       "   temp_bucket humidity_bucket   clutch_1 clutch_2 clutch_3  clutch_number  \\\n",
       "0   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "1   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "2   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "3   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "4   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "5   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "6   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "7   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "8   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "9   temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "10  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "11  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "12  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "13  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "14  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "15  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "16  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "17  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "18  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "19  temp_10-20      RH%_80-100 2013-07-22      NaT      NaT              1   \n",
       "\n",
       "    clutch egg_lay_date courting_date hatch_date dead_or_fledge_date  \n",
       "0      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "1      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "2      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "3      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "4      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "5      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "6      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "7      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "8      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "9      1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "10     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "11     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "12     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "13     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "14     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "15     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "16     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "17     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "18     1.0   2013-07-22    2013-06-21        NaT                 NaT  \n",
       "19     1.0   2013-07-22    2013-06-21        NaT                 NaT  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sensor_phase.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a clean up of dataframes that we'll no longer need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('{0} - Cleaning up intermediate data tables...'.format(str(time.ctime())), flush=True)\n",
    "del df_sensor_clutch\n",
    "del gb_lay_date\n",
    "del gb_hatch_date\n",
    "del gb_fledge_date\n",
    "del df_nest_joined\n",
    "del df_breeding_annual_stats\n",
    "del df_clutch_count\n",
    "del df_breeding_gb\n",
    "print('{0} - Done.'.format(str(time.ctime())), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickle the two key data files for use in later scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('{0} - Writing the final tables to pickle for future use...'.format(str(time.ctime())), flush=True)\n",
    "df_sensor_phase.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_sensor_phase.pkl'))\n",
    "df_nest_and_breeding.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_nest_and_breeding.pkl'))\n",
    "df_nest_static.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_nest_static.pkl'))\n",
    "print('{0} - Done.'.format(str(time.ctime())), flush=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
