{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1A Data Preparation\n",
    "This script performs the following tasks:\n",
    "1. Set up the environment\n",
    "2. Load the relevant data sets from file\n",
    " * `NestCharacteristic-Static.csv   -> df_nest_static`\n",
    " * `NestCharacteristic-Seasonal.csv -> df_nest_seasonal`\n",
    " * `BreedingDataCombined.csv        -> df_breeding`\n",
    " * `TempData_2_10_2016.txt          -> df_temp`\n",
    " * `HumidData_2_10_2016.txt         -> df_humd`\n",
    "3. Join them into a) a data file with nests, seasonal coverage and breeding observations and b) a data file for transactional sensor data\n",
    "4. Add additional computed features to the data\n",
    "5. Write the prepared data to file\n",
    " * `SensorDataWithBreedingPhase.csv` contains the temp and humidity logs, together with the breeding phase for that nest.\n",
    " * `NestDataWithBreedingStats.csv` contains all static nest masterdata, together with nest cover and breeding observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment\n",
    "### 1.1 Import the required libraries\n",
    "We need a certain set of common libraries for the tasks to be performed. These are imported below. If an import statement errors, you will need to install the library in your environment using the command line command `pip install <library>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment and variables...\n"
     ]
    }
   ],
   "source": [
    "print('Setting up environment and variables...', flush=True)\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# all the useful and reuseable functions are defined in helper_functions.py\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set up the variables\n",
    "You will need to change the values of the variables below to suit the names and directory location of your files to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update these with your file paths\n",
    "nest_static_file = os.path.normpath('./0_data/NestStaticDataTbl_access_27_01_2017.csv')\n",
    "nest_seasonal_file = os.path.normpath('./0_data/NestSeasonDataTbl_access_27_01_2017.csv')\n",
    "nest_annual_file = os.path.normpath('./0_data/NestAnnualDataTbl_access_28_01_2017.csv')\n",
    "breeding_data_file = os.path.normpath('./0_data/BreedingDataTbl_17_01_2017.csv')\n",
    "temperature_file = os.path.normpath('./0_data/TempData_2_10_2016.txt')\n",
    "humidity_file = os.path.normpath('./0_data/HumidData_2_10_2016.txt')\n",
    "\n",
    "# write intermediate tables to disk for debugging purposes\n",
    "write_temps = True\n",
    "df_sensor_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Set up helper functions\n",
    "Most helper functions are in helper_functions.py. These below are required to be in this module so they can use the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_temp_file(df, filepath, df_name):\n",
    "    '''\n",
    "    If write_temps is true, this function will write the specified Pandas dataframe (df) to csv at the specified location (filepath).\n",
    "    Variables:\n",
    "        df: a Pandas dataframe to be written to csv.\n",
    "        filepath: a string in Unix path format (using / not \\) for the csv destination.\n",
    "        df_name: human readable name or description of the dataframe for logging purposes.\n",
    "    '''\n",
    "    if write_temps:\n",
    "        print('{0} - Writing intermediate table {1} to disk.'.format(str(time.ctime()), df_name, filepath), flush=True)\n",
    "        df.to_csv(os.path.normpath(filepath))\n",
    "        if os.path.getsize(filepath) > 0:\n",
    "            print('{0} - Written {1}: {2:.3f} MB'.format(str(time.ctime()), filepath, os.path.getsize(filepath)/1000000), flush=True)\n",
    "            \n",
    "def breeding_year(date)   :\n",
    "    '''\n",
    "    Breeding year is defined as 01 Feb to 31 Jan using the year as at 01 Feb.\n",
    "    Arguments:\n",
    "        date is the datetime object representing the observation date to be converted into a breeding year.\n",
    "    Returns:\n",
    "        The Breeding Year for the provided date.\n",
    "    '''\n",
    "    if date.month == 1:\n",
    "        return date.year - 1\n",
    "    else:\n",
    "        return date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data from file\n",
    "### 2.1.1 Read in the NestCharacteristic-Static data (df_nest_static)\n",
    "This is the real nest master data to which everything else is joined. Refer to the GitHub Wiki for descriptions of the data fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 10:43:44 2017 - Loading the Nest Characteristic (Static) data file.\n",
      "Wed Feb  8 10:43:44 2017 - Nest Characteristic (Static) file is 0.039 MB.\n",
      "Wed Feb  8 10:43:44 2017 - Loading into memory.\n",
      "Wed Feb  8 10:43:44 2017 - Success: loaded 247 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(nest_static_file, 'Nest Characteristic (Static)')\n",
    "data_types = {'nest_id': str,\n",
    "              'nest_type': str,\n",
    "              'shape': str,\n",
    "              'easting': np.float32,\n",
    "              'northing': np.float32,\n",
    "              'elevation': np.float32,\n",
    "              'aspect': np.float32,\n",
    "              'slope': np.float32,\n",
    "              'treatment': str,\n",
    "              'duration_of_insolation': np.float32,\n",
    "              'box_height_mm': np.float32,\n",
    "              'box_length_mm': np.float32,\n",
    "              'box_width_mm': np.float32,\n",
    "              'box_wall_width_mm': np.float32,\n",
    "              'box_lid_depth': np.float32,\n",
    "              'entrance_bearing': np.float32,\n",
    "              'entrance_height': np.float32,\n",
    "              'entrance_length': np.float32,\n",
    "              'entrance_width': np.float32,\n",
    "              'vents': np.float32,\n",
    "              'tunnel': np.float32,\n",
    "              'distance_to_boardwalk_m': np.float32,\n",
    "              'distance_to_landfall_m': np.float32,\n",
    "              'distance_to_shore_vegetation_m': np.float32,\n",
    "              'box_vol_L': np.float32,\n",
    "              'box_area_cm2': np.float32,\n",
    "              'comment': str,\n",
    "              'lat': np.float32,\n",
    "              'long': np.float32,\n",
    "              'autoNumber': np.float32\n",
    "             }\n",
    "df_nest_static = pd.read_csv(nest_static_file, \n",
    "                             header=0,\n",
    "                             dtype=data_types,\n",
    "                             encoding='utf-8',\n",
    "                             error_bad_lines=True,\n",
    "                             warn_bad_lines=True)\n",
    "read_file_handler_end(nest_static_file, 'Nest Characteristic (Static)', df_nest_static, 'df_nest_static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Update and cleanse fields in NestCharacteristic-Static data (df_nest_static)\n",
    "* Make sure all the nest IDs are uppercase and trimmed\n",
    "* Create field `box_vol_L`\n",
    "* Create field `box_area_cm2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 10:52:56 2017 - Writing intermediate table df_nest_static to disk.\n",
      "Wed Feb  8 10:52:56 2017 - Written ./output/A_load_and_combine_data/df_nest_static.csv: 0.048 MB\n",
      "Wed Feb  8 10:52:56 2017 df_nest_static prepared successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the unwanted fields\n",
    "df_nest_static = df_nest_static[['nest_id', 'nest_type', 'shape', 'easting', 'northing', 'elevation',\n",
    "                                 'aspect', 'slope', 'treatment', 'duration_of_insolation',\n",
    "                                 'box_height_mm', 'box_length_mm', 'box_width_mm', 'box_wall_width_mm',\n",
    "                                 'box_lid_depth', 'entrance_bearing', 'entrance_height',\n",
    "                                 'entrance_length', 'entrance_width', 'vents', 'tunnel',\n",
    "                                 'distance_to_boardwalk_m', 'distance_to_landfall_m',\n",
    "                                 'distance_to_shore_vegetation_m', 'comment'\n",
    "                                ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_static['nest_id'] = df_nest_static['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# calc the volume\n",
    "# some boxes have only external measurements, not internal (which we are trying to calc). If it has external\n",
    "# but not internal, then use external measurements\n",
    "def box_vol_L(row):\n",
    "    # box measurements are external, so deduct walls. Boxes have no bottom, so deduct only lid depth from height.\n",
    "    return (row['box_length_mm'] - 2*row['box_wall_width_mm']) * (row['box_width_mm'] - 2*row['box_wall_width_mm']) * (row['box_height_mm']- row['box_lid_depth']) / 1000000\n",
    "df_nest_static['box_vol_L'] = df_nest_static.apply(box_vol_L, axis=1)\n",
    "        \n",
    "# calc the floor area\n",
    "def box_area_cm2(row):\n",
    "    return (row['box_length_mm'] - 2*row['box_wall_width_mm']) * (row['box_width_mm'] - 2*row['box_wall_width_mm']) / 100\n",
    "df_nest_static['box_area_cm2'] = df_nest_static.apply(box_area_cm2, axis=1)\n",
    "\n",
    "write_temp_file(df_nest_static, './output/A_load_and_combine_data/df_nest_static.csv', 'df_nest_static')\n",
    "print(str(time.ctime()), 'df_nest_static prepared successfully.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Read in the NestCharacteristic-Seasonal data (as df_nest_seasonal)\n",
    "Recorded for old boxes and natural nests. Contains seasonal observations of nest vegetation and cover.\n",
    "New boxes (not recorded) were an experiment in different building methods and their effect on box temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 11:18:49 2017 - Loading the Nest Characteristic (Seasonal) data file.\n",
      "Wed Feb  8 11:18:49 2017 - Nest Characteristic (Seasonal) file is 0.120 MB.\n",
      "Wed Feb  8 11:18:49 2017 - Loading into memory.\n",
      "Wed Feb  8 11:18:49 2017 - Success: loaded 1,929 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(nest_seasonal_file, 'Nest Characteristic (Seasonal)')\n",
    "\n",
    "data_types = {'type': str,\n",
    "              'nest_id': str,\n",
    "              'NestSeasYear': str,\n",
    "              'date': str,\n",
    "              'year': str,\n",
    "              'season': str,\n",
    "              'BoxCoverTotal': np.float32,\n",
    "              'BoxCoverDead': np.float32,\n",
    "              'BoxWood': np.float32,\n",
    "              'BoxWoodDead': np.float32,\n",
    "              'BoxVeg': np.float32,\n",
    "              'BoxVegDead': np.float32,\n",
    "              'QuadCoverTotal': np.float32,\n",
    "              'QuadCoverDead': np.float32,\n",
    "              'QuadWood': np.float32,\n",
    "              'QuadWoodDead': np.float32,\n",
    "              'QuadVeg': np.float32,\n",
    "              'QuadVegDead': np.float32,\n",
    "              'cavity_cover': np.float32,\n",
    "              'canopy_cover': np.float32,\n",
    "              'comments': str\n",
    "             }\n",
    "df_nest_seasonal = pd.read_csv(nest_seasonal_file,\n",
    "                               header=0,\n",
    "                               dtype=data_types,\n",
    "                               encoding='utf-8',\n",
    "                               parse_dates=['date'],\n",
    "                               dayfirst=True,\n",
    "                               error_bad_lines=True,\n",
    "                               warn_bad_lines=True)\n",
    "\n",
    "read_file_handler_end(nest_seasonal_file, 'Nest Characteristic (Seasonal)', df_nest_seasonal, 'df_nest_seasonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Update and cleanse fields in the NestCharacteristic-Seasonal data (df_nest_seasonal)\n",
    "* Nest IDs to be all uppercase and trimmed\n",
    "* recalculate the `year` and `season`\n",
    "* create the unique ID `BoxSeasYear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nest_id', 'year', 'season', 'month', 'BoxCoverTotal', 'BoxCoverDead',\n",
       "       'BoxWood', 'BoxWoodDead', 'BoxVeg', 'BoxVegDead', 'QuadCoverTotal',\n",
       "       'QuadCoverDead', 'QuadWood', 'QuadWoodDead', 'QuadVeg', 'QuadVegDead',\n",
       "       'comments', 'date', 'NestSeasYear', 'cavity_cover', 'canopy_cover'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nest_seasonal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 11:20:25 2017 - Writing intermediate table df_nest_seasonal to disk.\n",
      "Wed Feb  8 11:20:25 2017 - Written ./output/A_load_and_combine_data/df_nest_seasonal.csv: 0.158 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted fields: 'year', 'season', 'month',  'NestSeasYear'\n",
    "df_nest_seasonal = df_nest_seasonal[['nest_id', 'BoxCoverTotal', 'BoxCoverDead',\n",
    "                                     'BoxWood', 'BoxWoodDead', 'BoxVeg', 'BoxVegDead', 'QuadCoverTotal',\n",
    "                                     'QuadCoverDead', 'QuadWood', 'QuadWoodDead', 'QuadVeg', 'QuadVegDead',\n",
    "                                     'comments', 'date', 'cavity_cover', 'canopy_cover'\n",
    "                                    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_seasonal['nest_id'] = df_nest_seasonal['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# calculate the breeding year (Feb to Jan)\n",
    "df_nest_seasonal['breeding_year'] = df_nest_seasonal['date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# recalculate season (because was manually created). See helper_functions.py\n",
    "df_nest_seasonal['season'] = df_nest_seasonal['date'].apply(lambda x: season(x))\n",
    "\n",
    "# calc the unique ID\n",
    "df_nest_seasonal['NestSeasYear'] = df_nest_seasonal['nest_id'] + df_nest_seasonal['season'] + df_nest_seasonal['breeding_year'].apply(lambda x: str(x))\n",
    "\n",
    "# all blank canopy_covers should be 0 not NaN\n",
    "df_nest_seasonal['canopy_cover'] = df_nest_seasonal['canopy_cover'].apply(lambda x: 0 if np.isnan(x) else x)\n",
    "\n",
    "write_temp_file(df_nest_seasonal, './output/A_load_and_combine_data/df_nest_seasonal.csv', 'df_nest_seasonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Read in the Annual Nest Data file (as df_nest_annual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 12:34:36 2017 - Loading the Nest Characteristic (Annual) data file.\n",
      "Wed Feb  8 12:34:36 2017 - Nest Characteristic (Annual) file is 0.064 MB.\n",
      "Wed Feb  8 12:34:36 2017 - Loading into memory.\n",
      "Wed Feb  8 12:34:36 2017 - Success: loaded 822 records.\n"
     ]
    }
   ],
   "source": [
    "in_file = nest_annual_file\n",
    "desc = 'Nest Characteristic (Annual)'\n",
    "\n",
    "read_file_handler_start(in_file, desc)\n",
    "data_types = {\n",
    "    'nest_id': str,\n",
    "    'year': np.float32,\n",
    "    'nest_year': str,\n",
    "    'is_discrete_bush': str,\n",
    "    'vegetation_cover': np.float32,\n",
    "    'veg_cover_species': str,\n",
    "    'tetragonia': np.float32,\n",
    "    'rhagodia': np.float32,\n",
    "    'acacia': np.float32,\n",
    "    'bush_height': np.float32,\n",
    "    'budh_length': np.float32,\n",
    "    'bush_width': np.float32,\n",
    "    'bush_wall_depth': np.float32,\n",
    "    'cavity_height': np.float32,\n",
    "    'cavity_length': np.float32,\n",
    "    'cavity_width': np.float32,\n",
    "    'cavity_volume': np.float32,\n",
    "    'cavity_area': np.float32,\n",
    "    'entrance_bearing': np.float32,\n",
    "    'entrance_direction': str,\n",
    "    'entrance_height': np.float32,\n",
    "    'entrance_length': np.float32,\n",
    "    'entrance_width': np.float32,\n",
    "    'distance_to_nearest_neighbour_m': np.float32,\n",
    "    'notes': str,\n",
    "    'observation_date': str,\n",
    "}\n",
    "df_nest_annual = pd.read_csv(nest_annual_file,\n",
    "                          header=0, \n",
    "                          dtype=data_types,\n",
    "                          encoding='utf-8',\n",
    "                          parse_dates=['observation_date'],\n",
    "                          dayfirst=True,\n",
    "                          error_bad_lines=True,\n",
    "                          warn_bad_lines=True)\n",
    "\n",
    "read_file_handler_end(in_file, desc, df_nest_annual, 'df_nest_annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 12:34:38 2017 - Writing intermediate table df_nest_annual to disk.\n",
      "Wed Feb  8 12:34:38 2017 - Written ./output/A_load_and_combine_data/df_nest_annual.csv: 0.073 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted fields: 'year', 'nest_year', 'veg_cover_species', 'cavity_volume', 'cavity_area',\n",
    "df_nest_annual = df_nest_annual[[\n",
    "        'nest_id', 'is_discrete_bush', 'vegetation_cover',\n",
    "       'tetragonia', 'rhagodia', 'acacia', 'bush_height',\n",
    "       'budh_length', 'bush_width', 'bush_wall_depth', 'cavity_height',\n",
    "       'cavity_length', 'cavity_width', \n",
    "       'entrance_bearing', 'entrance_direction', 'entrance_height',\n",
    "       'entrance_length', 'entrance_width', 'distance_to_nearest_neighbour_m',\n",
    "       'notes', 'observation_date'\n",
    "    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_nest_annual['nest_id'] = df_nest_annual['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# recreate the veg_cover_species\n",
    "def veg_cover_species(row):\n",
    "    result = ''\n",
    "    if not np.isnan(row['tetragonia']) and row['tetragonia'] > 0:\n",
    "        result = 'Tetr '\n",
    "    if not np.isnan(row['rhagodia']) and row['rhagodia'] > 0:\n",
    "        result = '{0}{1}'.format(result, 'Rhag ')\n",
    "    if not np.isnan(row['acacia']) and row['acacia'] > 0:\n",
    "        result = '{0}{1}'.format(result, 'Acac ')\n",
    "    return result\n",
    "df_nest_annual['veg_cover_species'] = df_nest_annual.apply(veg_cover_species, axis=1)\n",
    "\n",
    "# calculate the breeding_year\n",
    "df_nest_annual['breeding_year'] = df_nest_annual['observation_date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# recalculate cavity_area in cm2. L, W, H are in mm.\n",
    "df_nest_annual['cavity_area_cm2'] = df_nest_annual['cavity_length'] * df_nest_annual['cavity_width'] / 100\n",
    "\n",
    "# recalculate cavity_volume in L, assumed a half ellipsoid. Vol of half ellipsoid is 1/2 * 3/4 * pi * abc where a,b,c are the radii\n",
    "df_nest_annual['cavity_volume_L'] = (0.5 * 0.75 * np.pi * \n",
    "                                     (df_nest_annual['cavity_length']/2) * \n",
    "                                     (df_nest_annual['cavity_width']/2) * \n",
    "                                     (df_nest_annual['cavity_height']/2)\n",
    "                                    ) / 1000000\n",
    "\n",
    "write_temp_file(df_nest_annual, './output/A_load_and_combine_data/df_nest_annual.csv', 'df_nest_annual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Read in the BreedingDataCombined file (as df_breeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 14:54:37 2017 - Loading the Breeding data file.\n",
      "Wed Feb  8 14:54:37 2017 - Breeding file is 0.711 MB.\n",
      "Wed Feb  8 14:54:37 2017 - Loading into memory.\n",
      "Wed Feb  8 14:54:40 2017 - Success: loaded 16,608 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(breeding_data_file, 'Breeding')\n",
    "data_types = {'nest_id': str,\n",
    "              'observation_date': str,\n",
    "              'Year': str,\n",
    "              'Month': str,\n",
    "              'ActivityStatus': np.float32,\n",
    "              'adult': np.float32,\n",
    "              'clutch': np.float32,\n",
    "              'eggs': np.float32,\n",
    "              'ChicksAlive': np.float32, # running obs, ignore\n",
    "              'ChicksDead': np.float32, # use sum\n",
    "              'TotalChicksHatch': np.float32, # use max\n",
    "              'ChicksAge': np.float32, # use max\n",
    "              'ChicksFledge': np.float32, # use max\n",
    "              'ChicksMissing': np.float32, # ignore\n",
    "              'ContentsNotVisible': np.float32, # ignore\n",
    "              'EggLayDate': str, # use max, avg or min\n",
    "              'IDChick1': np.float32,\n",
    "              'MassChick1': np.float32, # use max\n",
    "              'IDChick2': np.float32,\n",
    "              'MassChick2': np.float32, # use max\n",
    "              'comments': str\n",
    "             }\n",
    "df_breeding = pd.read_csv(breeding_data_file,\n",
    "                          header=0, \n",
    "                          dtype=data_types,\n",
    "                          encoding='utf-8',\n",
    "                          parse_dates=['observation_date', 'EggLayDate'],\n",
    "                          dayfirst=True,\n",
    "                          error_bad_lines=True,\n",
    "                          warn_bad_lines=True)\n",
    "read_file_handler_end(breeding_data_file, 'Breeding', df_breeding, 'df_breeding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Update and cleanse fields in the Breeding data (df_breeding)\n",
    "* Nest IDs to be all uppercase and trimmed\n",
    "* `year` is year of `observation_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 14:54:44 2017 - Writing intermediate table df_breeding to disk.\n",
      "Wed Feb  8 14:54:45 2017 - Written ./output/A_load_and_combine_data/df_breeding.csv: 0.951 MB\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted columns: 'Year', 'Month', 'ChicksAlive', 'ChicksMissing', 'ContentsNotVisible', \n",
    "df_breeding = df_breeding[[\n",
    "       'nest_id', 'observation_date', 'ActivityStatus',\n",
    "       'adult', 'clutch', 'eggs', 'ChicksDead',\n",
    "       'TotalChicksHatch', 'ChicksAge', 'ChicksFledge', \n",
    "       'EggLayDate', 'comments', 'IDChick1',\n",
    "       'MassChick1', 'IDChick2', 'MassChick2'\n",
    "    ]].copy()\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_breeding['nest_id'] = df_breeding['nest_id'].apply(lambda x: str(x)).apply(lambda x: x.strip()).apply(lambda x: x.upper())\n",
    "\n",
    "# create year field\n",
    "df_breeding['breeding_year'] = df_breeding['observation_date'].apply(lambda x: breeding_year(x))\n",
    "\n",
    "# convert the ActivityStatus into separate columns:\n",
    "# 0 - no activity at all\n",
    "# 1 - some evidence of nesting activity\n",
    "# 2 - breeding initiated (egg laid)\n",
    "# 3 - moulting activity\n",
    "df_breeding['used_for_nesting'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_breeding['used_for_breeding'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==2 else 0)\n",
    "df_breeding['used_for_moulting'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==3 else 0)\n",
    "df_breeding['used_for_nesting_or_breeding'] = df_breeding['ActivityStatus'].apply(lambda x: 1 if x==1 or x==2 else 0)\n",
    "\n",
    "write_temp_file(df_breeding, './output/A_load_and_combine_data/df_breeding.csv', 'df_breeding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Aggregate the Breeding data to get annual stats\n",
    "* **nest_id**\n",
    "* **breeding_year**\n",
    "* **clutch**\n",
    "* clutch_count\n",
    "* egg_count\n",
    "* chick_count\n",
    "* fletch_count\n",
    "* lay_date\n",
    "* age_at_fledging\n",
    "* mass_at_fledging_chick1\n",
    "* mass_at_fledging_chick2\n",
    "* chick_id1\n",
    "* chick_id2\n",
    "\n",
    "Add field:\n",
    "* `flag_activity_status`: True iff max(ActivityStatus) in year > 0. Note that ActivityStatus was not recorded for the numeric nest_ids, so this field should not be used for 'usage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nest_id', 'observation_date', 'ActivityStatus', 'adult', 'clutch',\n",
       "       'eggs', 'ChicksDead', 'TotalChicksHatch', 'ChicksAge', 'ChicksFledge',\n",
       "       'EggLayDate', 'comments', 'IDChick1', 'MassChick1', 'IDChick2',\n",
       "       'MassChick2', 'breeding_year', 'used_for_nesting', 'used_for_breeding',\n",
       "       'used_for_moulting', 'used_for_nesting_or_breeding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_breeding.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 14:54:52 2017 - Aggregating breeding data to get annual stats.\n",
      "Wed Feb  8 14:54:52 2017 - Writing intermediate table df_clutch_count to disk.\n",
      "Wed Feb  8 14:54:52 2017 - Written ./output/A_load_and_combine_data/df_clutch_count.csv: 0.015 MB\n",
      "Wed Feb  8 14:54:52 2017 - Writing intermediate table df_breeding_gb to disk.\n",
      "Wed Feb  8 14:54:52 2017 - Written ./output/A_load_and_combine_data/df_breeding_gb.csv: 0.016 MB\n"
     ]
    }
   ],
   "source": [
    "print('{0} - Aggregating breeding data to get annual stats.'.format(str(time.ctime())), flush=True)\n",
    "\n",
    "# get the clutches per nest and year\n",
    "# [[chosen columns]] -> groupby -> apply max -> add suffix -> remove multi-index\n",
    "df_clutch_count = df_breeding[['nest_id', 'breeding_year', 'clutch']\n",
    "                             ].groupby(['nest_id', 'breeding_year']).max().add_suffix('_count').reset_index()\n",
    "\n",
    "# get the annual stats per nest, year and clutch\n",
    "temp = df_breeding[['nest_id', 'breeding_year', 'ActivityStatus', 'clutch', 'eggs', 'ChicksDead', 'TotalChicksHatch',\n",
    "                    'ChicksFledge', 'EggLayDate', 'ChicksAge', 'MassChick1', 'MassChick2'\n",
    "                   ]].copy()\n",
    "aggregation = {\n",
    "    'eggs': {'egg_count': 'max'},\n",
    "    'ChicksDead': {'num_deaths': 'sum'}, \n",
    "    'TotalChicksHatch': {'num_hatched': 'max'},\n",
    "    'ChicksFledge': {'num_fledged': 'max'}, \n",
    "    'EggLayDate': {'EggLayDate': 'max'}, \n",
    "    'ChicksAge': {'age_at_fledge': 'max'},\n",
    "    'MassChick1': {'peak_mass_chick1': 'max'},\n",
    "    'MassChick2': {'peak_mass_chick2': 'max'}    \n",
    "}\n",
    "df_breeding_gb = temp.groupby(['nest_id', 'breeding_year', 'clutch']).agg(aggregation).reset_index()\n",
    "# df_breeding_gb is now a multi-index, which is a pain. Lose the top level with droplevel(), which loses the first 3 column names\n",
    "# Get them back by explicitly renaming.\n",
    "df_breeding_gb.columns = df_breeding_gb.columns.droplevel(0)\n",
    "df_breeding_gb.columns.values[0] = 'nest_id'\n",
    "df_breeding_gb.columns.values[1] = 'breeding_year'\n",
    "df_breeding_gb.columns.values[2] = 'clutch'\n",
    "df_breeding_gb['EggLayDate'] = df_breeding_gb['EggLayDate'].apply(lambda x: )\n",
    "\n",
    "write_temp_file(df_clutch_count, './output/A_load_and_combine_data/df_clutch_count.csv', 'df_clutch_count')\n",
    "write_temp_file(df_breeding_gb, './output/A_load_and_combine_data/df_breeding_gb.csv', 'df_breeding_gb')\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nest_id</th>\n",
       "      <th>breeding_year</th>\n",
       "      <th>clutch</th>\n",
       "      <th>egg_count</th>\n",
       "      <th>peak_mass_chick1</th>\n",
       "      <th>age_at_fledge</th>\n",
       "      <th>peak_mass_chick2</th>\n",
       "      <th>num_hatched</th>\n",
       "      <th>num_fledged</th>\n",
       "      <th>EggLayDate</th>\n",
       "      <th>num_deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.374451e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.411949e+18</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.374451e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>970.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.404518e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>101</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.433030e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>101</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.442966e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>101</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>825.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.465690e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>103</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.372550e+18</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.405642e+18</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nest_id  breeding_year  clutch  egg_count  peak_mass_chick1  age_at_fledge  \\\n",
       "0     100         2013.0     1.0        2.0               NaN            NaN   \n",
       "1     100         2014.0     1.0        2.0              60.0            2.0   \n",
       "2     100         2016.0     1.0        2.0               NaN            NaN   \n",
       "3     101         2013.0     1.0        2.0               NaN            NaN   \n",
       "4     101         2014.0     1.0        2.0            1010.0           60.0   \n",
       "5     101         2015.0     1.0        2.0            1380.0           53.0   \n",
       "6     101         2015.0     2.0        2.0             900.0           54.0   \n",
       "7     101         2016.0     1.0        2.0            1050.0           66.0   \n",
       "8     103         2013.0     1.0        2.0              70.0            2.0   \n",
       "9     103         2014.0     1.0        2.0              60.0            5.0   \n",
       "\n",
       "   peak_mass_chick2  num_hatched  num_fledged    EggLayDate  num_deaths  \n",
       "0               NaN          NaN          NaN  1.374451e+18         NaN  \n",
       "1              50.0          2.0          NaN  1.411949e+18         2.0  \n",
       "2               NaN          NaN          NaN           NaN         NaN  \n",
       "3               NaN          NaN          NaN  1.374451e+18         NaN  \n",
       "4             970.0          2.0          2.0  1.404518e+18         NaN  \n",
       "5               NaN          1.0          1.0  1.433030e+18         NaN  \n",
       "6             690.0          2.0          2.0  1.442966e+18         NaN  \n",
       "7             825.0          2.0          2.0  1.465690e+18         NaN  \n",
       "8              60.0          2.0          NaN  1.372550e+18         2.0  \n",
       "9              60.0          2.0          NaN  1.405642e+18         2.0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df_breeding_gb.copy()\n",
    "temp.columns = temp.columns.droplevel(0)\n",
    "\n",
    "temp.columns.values[0] = 'nest_id'\n",
    "temp.columns.values[1] = 'breeding_year'\n",
    "temp.columns.values[2] = 'clutch'\n",
    "\n",
    "temp['EggLayDate'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3744512e+18\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-bc6c0343d61f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EggLayDate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "a = temp['EggLayDate'][0]\n",
    "print(a)\n",
    "datetime.datetime.utcfromtimestamp(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Load the temperature data\n",
    "Read the temperature data file into memory and report on success/failure.\n",
    "We maintain a shortcut: if the joined temp-humidity output file (csv) already exists then skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 20:42:06 2017 - Loading the Temperature data file.\n",
      "Thu Jan 26 20:42:06 2017 - Temperature file is 88.172 MB.\n",
      "Thu Jan 26 20:42:06 2017 - Loading into memory. Please be patient.\n",
      "Thu Jan 26 20:42:20 2017 - Success: loaded 2,169,903 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(temperature_file, 'Temperature')\n",
    "column_names = ['recnum', 'datetime', 'temp_c', 'nest_id']\n",
    "data_types = {'recnum': np.int32, \n",
    "              'datetime': str, \n",
    "              'temp_c': np.float32, \n",
    "              'nest_id': str}\n",
    "df_temp = pd.read_csv(temperature_file,\n",
    "                      names=column_names,\n",
    "                      usecols=[0,1,2,3],\n",
    "                      dtype=data_types,\n",
    "#                           nrows=10000,\n",
    "                      parse_dates=['datetime'],\n",
    "                      infer_datetime_format=True,\n",
    "                      dayfirst=True,\n",
    "                      encoding='utf-8',\n",
    "                      error_bad_lines=True,\n",
    "                      warn_bad_lines=True\n",
    "                     )\n",
    "read_file_handler_end(temperature_file, 'Temperature', df_temp, 'df_temp')\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_temp['nest_id'] = df_temp['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 Load the humidity data\n",
    "Read the humidity data file into memory and report on success/failure.\n",
    "We maintain a shortcut: if the joined temp-humidity output file (csv) already exists then skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 20:42:21 2017 - Loading the Humidity data file.\n",
      "Thu Jan 26 20:42:21 2017 - Humidity file is 93.235 MB.\n",
      "Thu Jan 26 20:42:21 2017 - Loading into memory. Please be patient.\n",
      "Thu Jan 26 20:42:35 2017 - Success: loaded 2,173,732 records.\n"
     ]
    }
   ],
   "source": [
    "read_file_handler_start(humidity_file, 'Humidity')\n",
    "column_names = ['recnum', 'datetime', 'humidity', 'nest_id']\n",
    "data_types = {'recnum': np.int32, \n",
    "              'datetime': str, \n",
    "              'humidity': np.float32, \n",
    "              'nest_id': str}\n",
    "df_humd = pd.read_csv(humidity_file,\n",
    "                      names=column_names,\n",
    "                      usecols=[0,1,2,3],\n",
    "                      dtype=data_types,\n",
    "#                          nrows=10000,               # for testing only\n",
    "                      parse_dates=['datetime'],\n",
    "                      infer_datetime_format=True,\n",
    "                      dayfirst=True,\n",
    "                      encoding='utf-8',\n",
    "                      error_bad_lines=False,\n",
    "                      warn_bad_lines=True\n",
    "                     )\n",
    "\n",
    "read_file_handler_end(humidity_file, 'Humidity', df_humd, 'df_humd')\n",
    "\n",
    "# make sure the nest IDs are all caps\n",
    "df_humd['nest_id'] = df_humd['nest_id'].apply(lambda x: x.strip()).apply(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Join the loaded data\n",
    "### 3.1 Join the temperature and humidity data (creates df_sensor data)\n",
    "We maintain a shortcut: if the joined temp-humidity output file (csv) already exists then skip this step.\n",
    "Note that the same sensor records temp and humidity simultaneously, so the datetime stamps align and can be used in the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thu Jan 26 20:42:36 2017 - Joining the temperature and humidity data sets.\n",
      "Thu Jan 26 20:42:38 2017 - Join complete. Here are the stats:\n",
      "Records in temperature data:            2,169,903\n",
      "Records in humidity data:               2,173,732\n",
      "                              -------------------\n",
      "Records in joined data:                 2,173,738\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids:                           138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n{0} - Joining the temperature and humidity data sets.'.format(str(time.ctime())), flush=True)\n",
    "df_sensor_data = pd.merge(left=df_temp,\n",
    "                        right=df_humd,\n",
    "                        how='outer',\n",
    "                        on=['nest_id', 'datetime'], # both have same keys\n",
    "                        left_on=None, # same key names: don't need to specify R and L\n",
    "                        right_on=None, # same key names: don't need to specify R and L\n",
    "                        left_index=False, # dont' use left df index as key\n",
    "                        right_index=False, # dont' use right df index as key\n",
    "                        sort=True, # for efficiency do/not sort the df first\n",
    "                        suffixes=['_temp', '_humd']\n",
    "                        )[['nest_id', 'datetime', 'temp_c', 'humidity']] # take only these cols\n",
    "\n",
    "print('{0} - Join complete. Here are the stats:'.format(str(time.ctime())))\n",
    "print('Records in temperature data: {0:>20,}'.format(len(df_temp)))\n",
    "print('Records in humidity data:    {0:>20,}'.format(len(df_humd)))\n",
    "print('                              -------------------')\n",
    "print('Records in joined data:      {0:>20,}'.format(len(df_sensor_data)))\n",
    "print('\\nOverview:')\n",
    "gb = df_sensor_data.groupby(['nest_id'])\n",
    "print('Number of nest_ids:          {0:>20,}\\n'.format(len(gb)))\n",
    "del gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Calculations per-sensor reading\n",
    "The following calculations are added per sensor reading:\n",
    "* The `breeding_year`: same as the calendar year\n",
    "* `temp_bucket` is a category for each 5C temperature range: <0, 0-5, .., 60+\n",
    "* `humidity_bucket`: is a category for roughly 20% humidity ranges, based on human comfort zones\n",
    "* `average_activity_phase`: the average activity conducted at the time of the observation\n",
    "\n",
    "Note: An `actual_activity_phase` (the current phase of breeding based on per-nest observations) is added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 20:52:17 2017 - Calculating breeding year. Done.\n",
      "Thu Jan 26 20:52:27 2017 - Calculating temperature buckets. Done.\n",
      "Thu Jan 26 20:52:32 2017 - Calculating humidity buckets. Done.\n"
     ]
    }
   ],
   "source": [
    "# add the breeding_year (same as calendar year): \n",
    "print(str(time.ctime()), '- Calculating breeding year.', end='', flush=True)\n",
    "df_sensor_data['breeding_year'] = df_sensor_data['datetime'].apply(lambda x: x.year)\n",
    "print(' Done.', flush=True)\n",
    "\n",
    "# add the average breeding phases. Uses average_activity_phase() as defined in helper_functions.py\n",
    "# print(str(time.ctime()), '- Calculating average activity periods.', end='', flush=True)\n",
    "# df_sensor_data['average_activity_period'] = df_sensor_data['datetime'].apply(average_activity_phase)\n",
    "# print(' Done.', flush=True)\n",
    "\n",
    "# Add flags for various temperature ranges. Uses temp_bucket() as defined in helper_functions.py\n",
    "# These are summed to give the amount of time in the temp band\n",
    "print(str(time.ctime()), '- Calculating temperature buckets.', end='', flush=True)\n",
    "df_sensor_data['temp_bucket'] = df_sensor_data['temp_c'].apply(temp_bucket)\n",
    "print(' Done.', flush=True)\n",
    "\n",
    "# # Add flags for various humidity ranges. Uses humidity_bucket() as defined in helper_functions.py\n",
    "# # These are summed to give the amount of time in the humidity band\n",
    "print(str(time.ctime()), '- Calculating humidity buckets.', end='', flush=True)\n",
    "df_sensor_data['humidity_bucket'] = df_sensor_data['humidity'].apply(humidity_bucket)\n",
    "print(' Done.', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Join the NestCharacteristic Static and Seasonal data\n",
    "`df_nest_seasonal` + `df_nest_static` -> `df_nest_joined`\n",
    "\n",
    "Seasonal is `left` and Static is `right`, such that the Seasonal data is augmented with the nests static metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 21:13:48 2017 Joining the Nest Characteristic (Seasonal and Static) data sets .\n",
      "Thu Jan 26 21:13:48 2017 - Join complete. Here are the stats:\n",
      "Records in seasonal data:                   1,711\n",
      "Records in static data:                       241\n",
      "                              -------------------\n",
      "Records in joined data:                     1,711\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids:                           193\n",
      "Thu Jan 26 21:13:48 2017 - Writing intermediate table df_nest_joined to disk.\n",
      "Thu Jan 26 21:13:48 2017 - Written ./output/A_load_and_combine_data/df_nest_joined.csv: 0.452 MB\n"
     ]
    }
   ],
   "source": [
    "print('{0} Joining the Nest Characteristic (Seasonal and Static) data sets .'.format(str(time.ctime())), flush=True)\n",
    "df_nest_joined = pd.merge(left=df_nest_seasonal,\n",
    "                            right=df_nest_static,\n",
    "                            how='left',\n",
    "                            on=['nest_id'], # both have same keys\n",
    "                            left_on=None, # same key names: don't need to specify R and L\n",
    "                            right_on=None, # same key names: don't need to specify R and L\n",
    "                            left_index=False, # dont' use left df index as key\n",
    "                            right_index=False, # dont' use right df index as key\n",
    "                            sort=True, # for efficiency do/not sort the df first\n",
    "                            suffixes=['_seasonal', '_static']\n",
    "                            )\n",
    "if df_nest_joined is not None:\n",
    "    print('{0} - Join complete. Here are the stats:'.format(str(time.ctime())))\n",
    "    print('Records in seasonal data:    {0:>20,}'.format(len(df_nest_seasonal)))\n",
    "    print('Records in static data:      {0:>20,}'.format(len(df_nest_static)))\n",
    "    print('                              -------------------')\n",
    "    print('Records in joined data:      {0:>20,}'.format(len(df_nest_joined)))\n",
    "    print('\\nOverview:')\n",
    "    gb = df_nest_joined.groupby(['nest_id'])\n",
    "    print('Number of nest_ids:          {0:>20,}'.format(len(gb)))\n",
    "    write_temp_file(df_nest_joined, './output/A_load_and_combine_data/df_nest_joined.csv', 'df_nest_joined')\n",
    "    del gb\n",
    "else:\n",
    "    print('{0} - JOIN FAILED!!!.'.format(str(time.ctime())), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Join the breeding stats together\n",
    "`df_breeding_gb + df_clutch_count -> df_breeding_annual_stats`\n",
    "\n",
    "Clutch counts per year and annual clutch survival stats.\n",
    "Note that the reduced record count in `df_breeding_annual_stats` compared to `df_clutch_count` is due to a number of nest-years having blank/zero clutches. These are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thu Jan 26 21:13:58 2017 - Merging the aggregated breeding stats.\n",
      "Thu Jan 26 21:13:58 2017 - Join complete. Here are the stats:\n",
      "Records in annual stats data:                  267\n",
      "Records in clutch count data:                  302\n",
      "                              -------------------\n",
      "Records in joined data:                        267\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids in clutch count:            129\n",
      "Number of nest_ids in breeding stats:          121\n",
      "Number of nest_ids in joined:                  121\n",
      "Thu Jan 26 21:13:58 2017 - Writing intermediate table df_breeding_annual_stats to disk.\n",
      "Thu Jan 26 21:13:58 2017 - Written ./output/A_load_and_combine_data/df_breeding_annual_stats.csv: 0.016 MB\n"
     ]
    }
   ],
   "source": [
    "# join the clutch count on to the annual stats\n",
    "print('\\n{0} - Merging the aggregated breeding stats.'.format(str(time.ctime())), flush=True)\n",
    "df_breeding_annual_stats = pd.merge(left=df_breeding_gb,\n",
    "                                    right=df_clutch_count,\n",
    "                                    how='left',\n",
    "                                    on=['nest_id', 'year'], # both have same keys\n",
    "                                    sort=True # for efficiency do/not sort the df first\n",
    "                                   )\n",
    "\n",
    "print('{0} - Join complete. Here are the stats:'.format(str(time.ctime())))\n",
    "print('Records in annual stats data: {0:>20,}'.format(len(df_breeding_gb)))\n",
    "print('Records in clutch count data: {0:>20,}'.format(len(df_clutch_count)))\n",
    "print('                              -------------------')\n",
    "print('Records in joined data:       {0:>20,}'.format(len(df_breeding_annual_stats)))\n",
    "print('\\nOverview:')\n",
    "gb = df_breeding_annual_stats.groupby(['nest_id'])\n",
    "print('Number of nest_ids in clutch count:   {0:>12,}'.format(len(df_clutch_count.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in breeding stats: {0:>12,}'.format(len(df_breeding_gb.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in joined:         {0:>12,}'.format(len(df_breeding_annual_stats.groupby(['nest_id']))))\n",
    "write_temp_file(df_breeding_annual_stats, './output/A_load_and_combine_data/df_breeding_annual_stats.csv', 'df_breeding_annual_stats')\n",
    "del gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Join the Nest data (seasonal and static) to the Breeding stats\n",
    "`df_nest_joined + df_breeding_annual_stats -> df_nest_and_breeding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thu Jan 26 21:13:58 2017 - Merging the aggregated breeding stats to the static and seasonal nest data.\n",
      "Thu Jan 26 21:13:58 2017 - Join complete. Here are the stats:\n",
      "Records in nest data:                        1,711\n",
      "Records in breeding stats data:                267\n",
      "                                      ------------\n",
      "Records in joined data:                      1,829\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids in nest data:               193\n",
      "Number of nest_ids in breeding stats:          121\n",
      "Number of nest_ids in joined:                  193\n",
      "Thu Jan 26 21:13:58 2017 - Writing intermediate table df_nest_and_breeding to disk.\n",
      "Thu Jan 26 21:13:59 2017 - Written ./output/A_load_and_combine_data/NestDataWithBreedingStats.csv: 0.525 MB\n"
     ]
    }
   ],
   "source": [
    "# join the annual clutch and breeding stats onto the full seasonal and static nest data\n",
    "print('\\n{0} - Merging the aggregated breeding stats to the static and seasonal nest data.'.format(str(time.ctime())), flush=True)\n",
    "df_nest_and_breeding = pd.merge(left=df_nest_joined,\n",
    "                                right=df_breeding_annual_stats,\n",
    "                                how='left',\n",
    "                                on=['nest_id','year'], # both have same keys\n",
    "                                sort=True \n",
    "                               )\n",
    "\n",
    "print('{0} - Join complete. Here are the stats:'.format(str(time.ctime())))\n",
    "print('Records in nest data:                 {0:>12,}'.format(len(df_nest_joined)))\n",
    "print('Records in breeding stats data:       {0:>12,}'.format(len(df_breeding_annual_stats)))\n",
    "print('                                      ------------')\n",
    "print('Records in joined data:               {0:>12,}'.format(len(df_nest_and_breeding)))\n",
    "print('\\nOverview:')\n",
    "gb = df_breeding_annual_stats.groupby(['nest_id'])\n",
    "print('Number of nest_ids in nest data:      {0:>12,}'.format(len(df_nest_joined.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in breeding stats: {0:>12,}'.format(len(df_breeding_annual_stats.groupby(['nest_id']))))\n",
    "print('Number of nest_ids in joined:         {0:>12,}'.format(len(df_nest_and_breeding.groupby(['nest_id']))))\n",
    "write_temp_file(df_nest_and_breeding, './output/A_load_and_combine_data/NestDataWithBreedingStats.csv', 'df_nest_and_breeding')\n",
    "del gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Aggregate the sensor data into stats per breeding phase\n",
    "To understand the effect of nest conditions (from sensor data) in the choice of nest and breeding success of the nest, we need to break up the stats into:\n",
    "* *annual stats* which represent the averages, spikes etc for the entire year. These give an understanding of the nest itself.\n",
    "* *phase stats* which represent the conditions during specific phases of the breeding cycle. E.g. during nesting, during incubation, during rearing. To get these phase stats, we need to get the phase boundary dates from the breeding observation data.\n",
    "\n",
    "The nest sensor readings are aggregated to summarise the nest conditions by `nest`, `breeding_year` and `activity_phase`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5.1 Get the actual breeding phase dates\n",
    "Summarise the breeding data to obtain the following:\n",
    "* list of all nests (regardless of breeding activity)\n",
    "* the `nesting_date` for each nest in each year\n",
    "* the `egg_lay_date` for each nest, year and clutch\n",
    "* the `hatch_date` for each nest, year and clutch\n",
    "* the `fledge_date` for each nest, year and clutch\n",
    "\n",
    "Join these all back together to get the phase dates all in one place, then join the combined result on to the sensor data table and calculate the phase in which each sensor reading occurred.\n",
    "This will take a while.\n",
    "\n",
    "**Issue: Nesting dates dont work: the second clutch will have first nesting date and the first obs for many nests is after the lay date, so nesting_date > lay_date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 21:13:59 2017 - Calculating the breeding phase dates for each nest and year.\n",
      "Thu Jan 26 21:13:59 2017 - Writing intermediate table df_all_nests to disk.\n",
      "Thu Jan 26 21:13:59 2017 - Written ./output/A_load_and_combine_data/df_all_nests.csv: 0.002 MB\n",
      "Thu Jan 26 21:13:59 2017 - Writing intermediate table gb_lay_date to disk.\n",
      "Thu Jan 26 21:13:59 2017 - Written ./output/A_load_and_combine_data/gb_lay_date.csv: 0.010 MB\n",
      "Thu Jan 26 21:13:59 2017 - Writing intermediate table gb_hatch_date to disk.\n",
      "Thu Jan 26 21:13:59 2017 - Written ./output/A_load_and_combine_data/gb_hatch_date.csv: 0.006 MB\n",
      "Thu Jan 26 21:13:59 2017 - Writing intermediate table gb_fledge_date to disk.\n",
      "Thu Jan 26 21:13:59 2017 - Written ./output/A_load_and_combine_data/gb_fledge_date.csv: 0.003 MB\n",
      "Thu Jan 26 21:13:59 2017 - Merging the phase date tables.\n",
      "Thu Jan 26 21:13:59 2017 - Writing intermediate table df_phase_dates to disk.\n",
      "Thu Jan 26 21:13:59 2017 - Written ./output/A_load_and_combine_data/df_phase_dates.csv: 0.016 MB\n",
      "Thu Jan 26 21:13:59 2017 - Pivot breeding data to get the clutch dates.\n",
      "Thu Jan 26 21:13:59 2017 - Writing intermediate table df_clutch_pivot to disk.\n",
      "Thu Jan 26 21:13:59 2017 - Written ./output/A_load_and_combine_data/df_clutch_pivot.csv: 0.006 MB\n"
     ]
    }
   ],
   "source": [
    "# for each nest, year and clutch, get the following:\n",
    "# first activity_status date (nesting_date), EggLayDate, hatch_date, fledge_date\n",
    "# nesting_date, hatch_date, fledge_date are the min observation_date per nest, year, clutch where the value is not NaN\n",
    "\n",
    "print('{0} - Calculating the breeding phase dates for each nest and year.'.format(str(time.ctime())), flush=True)\n",
    "# all observed nests\n",
    "df_all_nests = df_nest_static[['nest_id']].drop_duplicates()\n",
    "write_temp_file(df_all_nests, './output/A_load_and_combine_data/df_all_nests.csv', 'df_all_nests')\n",
    "\n",
    "# egg_lay_date\n",
    "gb_lay_date = df_breeding[['nest_id', 'year', 'clutch', 'EggLayDate']\n",
    "                         ].groupby(['nest_id', 'year', 'clutch']).min().reset_index()\n",
    "gb_lay_date.rename(columns={'EggLayDate': 'egg_lay_date'}, inplace=True)\n",
    "# nesting date: 31 days before egg_lay_date\n",
    "gb_lay_date['courting_date'] = gb_lay_date['egg_lay_date'] - datetime.timedelta(days=31)\n",
    "write_temp_file(gb_lay_date, './output/A_load_and_combine_data/gb_lay_date.csv', 'gb_lay_date')\n",
    "\n",
    "# hatch_date\n",
    "def hatch_date(row):\n",
    "    return row['observation_date'] - datetime.timedelta(days=row['ChicksAge'])\n",
    "# get the observation date (select columns)                                                 where age is not blank (i.e. they're there)\n",
    "gb_hatch_date = df_breeding[['nest_id', 'year', 'clutch', 'observation_date', 'ChicksAge']].loc[df_breeding['ChicksAge'].notnull()]\n",
    "gb_hatch_date['hatch_date'] = gb_hatch_date.apply(hatch_date, axis=1)\n",
    "# get the min hatch_date \n",
    "gb_hatch_date = gb_hatch_date[['nest_id', 'year', 'clutch', 'hatch_date']].groupby(['nest_id', 'year', 'clutch']).min().reset_index()\n",
    "write_temp_file(gb_hatch_date, './output/A_load_and_combine_data/gb_hatch_date.csv', 'gb_hatch_date')\n",
    "\n",
    "# fledge_date\n",
    "# is either the date that the chicks were of age and no longer observed in the nest, or were observed dead\n",
    "# get the observation date (select columns) where there is a fledge flag\n",
    "gb_fledge_date = df_breeding[['nest_id', 'year', 'clutch', 'observation_date', 'ChicksAlive', 'ChicksDead', 'ChicksFledge']].fillna(0)\n",
    "gb_fledge_date['dead_or_fledged'] = gb_fledge_date.apply(lambda row: row['ChicksFledge'] > 0 or (row['ChicksDead'] > 0 and row['ChicksAlive'] == 0), axis=1)\n",
    "gb_fledge_date = gb_fledge_date.query('dead_or_fledged')\n",
    "# get the min obs date, which is the earliest fledge recording (per clutch)\n",
    "gb_fledge_date = gb_fledge_date[['nest_id', 'year', 'clutch', 'observation_date']].groupby(['nest_id', 'year', 'clutch']).min().reset_index()\n",
    "# rename the obs date \n",
    "gb_fledge_date.rename(columns={'observation_date': 'dead_or_fledge_date'}, inplace=True)\n",
    "write_temp_file(gb_fledge_date, './output/A_load_and_combine_data/gb_fledge_date.csv', 'gb_fledge_date')\n",
    "\n",
    "# join the key date tables together\n",
    "print('{0} - Merging the phase date tables.'.format(str(time.ctime())), flush=True)\n",
    "df_phase_dates = pd.merge(left=df_all_nests, right=gb_lay_date, how='left', on=['nest_id'], sort=True)\n",
    "df_phase_dates = pd.merge(left=df_phase_dates, right=gb_hatch_date, how='left', on=['nest_id', 'year', 'clutch'], sort=True)\n",
    "df_phase_dates = pd.merge(left=df_phase_dates, right=gb_fledge_date, how='left', on=['nest_id', 'year', 'clutch'], sort=True)\n",
    "write_temp_file(df_phase_dates, './output/A_load_and_combine_data/df_phase_dates.csv', 'df_phase_dates')\n",
    "\n",
    "print('{0} - Pivot breeding data to get the clutch dates.'.format(str(time.ctime())), flush=True)\n",
    "# get the required cols\n",
    "df_clutch_pivot = gb_lay_date[['nest_id', 'year', 'clutch', 'egg_lay_date']].copy()\n",
    "# we have to combine the index because pivot() does not like a multi-index\n",
    "df_clutch_pivot['nestyear'] = df_clutch_pivot['nest_id'] + '-' + df_clutch_pivot['year'].apply(lambda x: str(x))\n",
    "# drop the old index fields\n",
    "df_clutch_pivot = df_clutch_pivot[['nestyear', 'clutch', 'egg_lay_date']]\n",
    "# do the pivot to get the (up to three) clutch dates per nest and year\n",
    "df_clutch_pivot = df_clutch_pivot.pivot(index='nestyear', columns='clutch')['egg_lay_date'].reset_index()\n",
    "# rename and restore the indexes\n",
    "df_clutch_pivot.rename(columns={1.0: 'clutch_1', 2.0: 'clutch_2', 3.0: 'clutch_3'}, inplace=True)\n",
    "df_clutch_pivot['nest_id'] = df_clutch_pivot['nestyear'].apply(lambda x: x.split('-')[0])\n",
    "df_clutch_pivot['breeding_year'] = df_clutch_pivot['nestyear'].apply(lambda x: int(x.split('-')[1]))\n",
    "df_clutch_pivot = df_clutch_pivot[['nest_id', 'breeding_year', 'clutch_1', 'clutch_2', 'clutch_3']]\n",
    "write_temp_file(df_clutch_pivot, './output/A_load_and_combine_data/df_clutch_pivot.csv', 'df_clutch_pivot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add clutch dates to the sensor data** \n",
    "\n",
    "The `sensor_data` are lacking a `clutch` number, which will create duplicates if we attempt to join on the phase dates. Get the clutch dates and join them into the `sensor_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 21:13:59 2017 - Join the clutch dates to the sensor data.\n",
      "Thu Jan 26 21:14:00 2017 - Done. Rows: 2,173,738\n",
      "Thu Jan 26 21:14:00 2017 - Assigning a clutch number to each sensor record. Be patient.\n",
      "Thu Jan 26 21:17:55 2017 - Done.\n",
      "Thu Jan 26 21:17:55 2017 - Writing intermediate table df_sensor_clutch to disk.\n",
      "Thu Jan 26 21:18:26 2017 - Written ./output/A_load_and_combine_data/df_sensor_clutch.csv: 258.504 MB\n"
     ]
    }
   ],
   "source": [
    "# to avoid making epic dupes, we need to first add the clutch number on to the sensor data table\n",
    "\n",
    "print('{0} - Join the clutch dates to the sensor data.'.format(str(time.ctime())), flush=True)\n",
    "# join on to the sensor data\n",
    "df_sensor_clutch = pd.merge(left=df_sensor_data, \n",
    "                            right=df_clutch_pivot, \n",
    "                            how='left', \n",
    "                            on=['nest_id', 'breeding_year'],\n",
    "                            sort=True\n",
    "                           )\n",
    "print('{0} - Done. Rows: {1:,}'.format(str(time.ctime()), len(df_sensor_clutch)), flush=True)\n",
    "\n",
    "print('{0} - Assigning a clutch number to each sensor record. Be patient.'.format(str(time.ctime())), flush=True)\n",
    "# flag each reading with a clutch number\n",
    "def clutch_number(row):\n",
    "    if pd.isnull(row['clutch_1']):\n",
    "        # there are no breeding observations for this nest and year\n",
    "        return 0\n",
    "    else:\n",
    "        # there is at least 1 clutch\n",
    "        if pd.isnull(row['clutch_2']) or row['datetime'] < row['clutch_2']:\n",
    "            # there was only a single clutch, or there were >1 but this reading was before the 2nd clutch\n",
    "            return 1\n",
    "        elif pd.isnull(row['clutch_3']) or (not pd.isnull(row['clutch_3']) and row['datetime'] < row['clutch_3']):\n",
    "            # there is a 2nd clutch if we got this far. if there is no 3rd, or the reading is before the 3rd, then this is 2nd\n",
    "            return 2\n",
    "        else:\n",
    "            # there is a 3rd clutch and the sensor reading is after the 3rd\n",
    "            return 3\n",
    "\n",
    "df_sensor_clutch['clutch_number'] = df_sensor_clutch.apply(lambda row: clutch_number(row), axis=1)\n",
    "print('{0} - Done.'.format(str(time.ctime())), flush=True)\n",
    "write_temp_file(df_sensor_clutch, './output/A_load_and_combine_data/df_sensor_clutch.csv', 'df_sensor_clutch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the actual breeding phase dates on to the sensor data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 21:18:26 2017 - Join the phase dates on to the sensor data.\n",
      "Thu Jan 26 21:18:28 2017 - Done. Rows: 2,173,738\n"
     ]
    }
   ],
   "source": [
    "print('{0} - Join the phase dates on to the sensor data.'.format(str(time.ctime())), flush=True)\n",
    "df_sensor_phase = pd.merge(left=df_sensor_clutch,\n",
    "                        right=df_phase_dates,\n",
    "                        how='left',\n",
    "                        left_on=['nest_id', 'breeding_year', 'clutch_number'], # same key names: don't need to specify R and L\n",
    "                        right_on=['nest_id', 'year', 'clutch'], # same key names: don't need to specify R and L\n",
    "                        sort=True # for efficiency do/not sort the df first\n",
    "#                             suffixes=['_temp', '_humd']\n",
    "                        )\n",
    "print('{0} - Done. Rows: {1:,}'.format(str(time.ctime()), len(df_sensor_phase)), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a clean up of dataframes that we'll no longer need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 21:18:28 2017 - Cleaning up intermediate data tables...\n",
      "Thu Jan 26 21:18:28 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "print('{0} - Cleaning up intermediate data tables...'.format(str(time.ctime())), flush=True)\n",
    "del df_sensor_clutch\n",
    "del gb_lay_date\n",
    "del gb_hatch_date\n",
    "del gb_fledge_date\n",
    "del df_nest_joined\n",
    "del df_breeding_annual_stats\n",
    "del df_clutch_count\n",
    "del df_breeding_gb\n",
    "print('{0} - Done.'.format(str(time.ctime())), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickle the two key data files for use in later scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 26 21:20:31 2017 - Writing the final tables to pickle for future use...\n",
      "Thu Jan 26 21:20:37 2017 - Done.\n"
     ]
    }
   ],
   "source": [
    "print('{0} - Writing the final tables to pickle for future use...'.format(str(time.ctime())), flush=True)\n",
    "df_sensor_phase.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_sensor_phase.pkl'))\n",
    "df_nest_and_breeding.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_nest_and_breeding.pkl'))\n",
    "df_nest_static.to_pickle(os.path.normpath('./output/A_load_and_combine_data/df_nest_static.pkl'))\n",
    "print('{0} - Done.'.format(str(time.ctime())), flush=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
