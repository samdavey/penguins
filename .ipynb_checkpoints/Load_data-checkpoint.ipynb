{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "The following need to be done:\n",
    "* Load the relevant data sets from file\n",
    "* Join them into a single data set\n",
    "* Add additional computed features to the data\n",
    "* Write the prepared data to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "We need a certain set of common libraries for the tasks to be performed. These are imported below. If an import statement errors, you will need to install the library in your environment using the command line command `pip install <library>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import yaml\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the variables\n",
    "Change the values of the variables below to suit the files (names and directory location) to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Currently in unix format as docker containers run on debian\n",
    "config_file = os.path.normpath('./config.yml')\n",
    "temperature_file = os.path.normpath('./data/TempData_2_10_2016.txt')\n",
    "humidity_file = os.path.normpath('./data/HumidData_2_10_2016.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialise the config.yml file\n",
    "with open(config_file, 'r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set the database connection parameters based on the config.ini file\n",
    "host = config['PostgreSQL']['host']\n",
    "port = config['PostgreSQL']['port']\n",
    "dbname = config['PostgreSQL']['dbname']\n",
    "user  = config['PostgreSQL']['user']\n",
    "password = config['PostgreSQL']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#establish connection to the postgres database using the generated connection string\n",
    "engine = create_engine(r\"postgresql://\"+user+\":\"+password+\"@\"+host+\"/\"+dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the temperature data\n",
    "Read the temperature data file into memory and report on success/failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading temperature file into memory.\n",
      "File is 88.2 MB.\n",
      "Loading into memory. Please be patient. \n",
      "Success: loaded 2,169,903 records.\n"
     ]
    }
   ],
   "source": [
    "column_names = ['recnum', 'datetime', 'temp_c', 'nest_id']\n",
    "data_types = {'recnum': np.int32, \n",
    "              'datetime': str, \n",
    "              'temp_c': np.float32, \n",
    "              'nest_id': str}\n",
    "file_size = os.path.getsize(temperature_file)\n",
    "print('\\nLoading temperature file into memory.\\nFile is {0:.1f} MB.'.format((file_size/1000000)))\n",
    "\n",
    "if file_size > 5000000: # over 5mb\n",
    "    print('Loading into memory. Please be patient. ', flush=True)\n",
    "else:\n",
    "    print('Loading into memory. ', flush=True, end='')\n",
    "    \n",
    "df_temp = pd.read_csv(temperature_file,\n",
    "                     names=column_names,\n",
    "                     usecols=[0,1,2,3],\n",
    "                     dtype=data_types,\n",
    "#                      nrows=2048,               # for testing only\n",
    "                      parse_dates=['datetime'],\n",
    "                      dayfirst=True,\n",
    "                      encoding='utf-8',\n",
    "                      error_bad_lines=False,\n",
    "                      warn_bad_lines=True\n",
    "                     )\n",
    "\n",
    "if df_temp is not None:\n",
    "    print('Success: loaded {0:,} records.'.format(len(df_temp)))\n",
    "else:\n",
    "    print('### FAILED! ###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading humidity file into memory.\n",
      "File is 93.2 MB.\n",
      "Loading into memory. Please be patient. \n",
      "Success: loaded 2,173,732 records.\n"
     ]
    }
   ],
   "source": [
    "column_names = ['recnum', 'datetime', 'humidity', 'nest_id']\n",
    "data_types = {'recnum': np.int32, \n",
    "              'datetime': str, \n",
    "              'humidity': np.float32, \n",
    "              'nest_id': str}\n",
    "file_size = os.path.getsize(humidity_file)\n",
    "print('\\nLoading humidity file into memory.\\nFile is {0:.1f} MB.'.format((file_size/1000000)))\n",
    "\n",
    "if file_size > 5000000: # over 5mb\n",
    "    print('Loading into memory. Please be patient. ', flush=True)\n",
    "else:\n",
    "    print('Loading into memory. ', flush=True, end='')\n",
    "\n",
    "df_humd = pd.read_csv(humidity_file,\n",
    "                     names=column_names,\n",
    "                     usecols=[0,1,2,3],\n",
    "                     dtype=data_types,\n",
    "#                      nrows=2048,               # for testing only\n",
    "                      parse_dates=['datetime'],\n",
    "                      dayfirst=True,\n",
    "                      encoding='utf-8',\n",
    "                      error_bad_lines=False,\n",
    "                      warn_bad_lines=True\n",
    "                     )\n",
    "\n",
    "if df_humd is not None:\n",
    "    print('Success: loaded {0:,} records.'.format(len(df_humd)))\n",
    "else:\n",
    "    print('### FAILED! ###')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------\n",
    "# Dev and Test\n",
    "### ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in temperature data:            2,169,903\n",
      "Records in humidity data:               2,173,732\n",
      "                              -------------------\n",
      "Records in joined data:                 2,173,738\n",
      "\n",
      "Overview:\n",
      "Number of nest_ids:                           140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nest_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">101</th>\n",
       "      <th>2013-07-11 21:49:00</th>\n",
       "      <th>15.10</th>\n",
       "      <th>91.949997</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 22:04:00</th>\n",
       "      <th>15.10</th>\n",
       "      <th>91.949997</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 22:19:00</th>\n",
       "      <th>15.61</th>\n",
       "      <th>91.519997</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 22:34:00</th>\n",
       "      <th>15.61</th>\n",
       "      <th>91.519997</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 22:49:00</th>\n",
       "      <th>15.61</th>\n",
       "      <th>91.519997</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 23:04:00</th>\n",
       "      <th>15.61</th>\n",
       "      <th>91.080002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 23:19:00</th>\n",
       "      <th>15.61</th>\n",
       "      <th>91.080002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 23:34:00</th>\n",
       "      <th>15.61</th>\n",
       "      <th>91.080002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11 23:49:00</th>\n",
       "      <th>15.61</th>\n",
       "      <th>91.080002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-12 00:04:00</th>\n",
       "      <th>15.10</th>\n",
       "      <th>91.080002</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(101, 2013-07-11 21:49:00, 15.100000381469727, 91.94999694824219), (101, 2013-07-11 22:04:00, 15.100000381469727, 91.94999694824219), (101, 2013-07-11 22:19:00, 15.609999656677246, 91.5199966430664), (101, 2013-07-11 22:34:00, 15.609999656677246, 91.5199966430664), (101, 2013-07-11 22:49:00, 15.609999656677246, 91.5199966430664), (101, 2013-07-11 23:04:00, 15.609999656677246, 91.08000183105469), (101, 2013-07-11 23:19:00, 15.609999656677246, 91.08000183105469), (101, 2013-07-11 23:34:00, 15.609999656677246, 91.08000183105469), (101, 2013-07-11 23:49:00, 15.609999656677246, 91.08000183105469), (101, 2013-07-12 00:04:00, 15.100000381469727, 91.08000183105469)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = pd.merge(left=df_temp,\n",
    "                        right=df_humd,\n",
    "                        how='outer',\n",
    "                        on=['nest_id', 'datetime'], # both have same keys\n",
    "                        left_on=None, # same key names: don't need to specify R and L\n",
    "                        right_on=None, # same key names: don't need to specify R and L\n",
    "                        left_index=False, # dont' use left df index as key\n",
    "                        right_index=False, # dont' use right df index as key\n",
    "                        sort=True, # for efficiency do/not sort the df first\n",
    "                        suffixes=['_temp', '_humd']\n",
    "                        )[['nest_id', 'datetime', 'temp_c', 'humidity']] # take only these cols\n",
    "\n",
    "print('Records in temperature data: {0:>20,}'.format(len(df_temp)))\n",
    "print('Records in humidity data:    {0:>20,}'.format(len(df_humd)))\n",
    "print('                              -------------------')\n",
    "print('Records in joined data:      {0:>20,}'.format(len(joined)))\n",
    "print('\\nOverview:')\n",
    "\"\"\"\n",
    "Have to add a function to the end of a groupby statement otherwise it becomes an un-uploadable pandas 'groupby object', \n",
    "instead of a normal dataframe. Documentation is sparse but my theory is it because of python's lack of types. So it's like\n",
    "you're grouping numeric or float fields in SQL without specifying the grouping method (e.g. count()).\n",
    "Except in pandas if you forget to do that it just gives you a 'shitty version' dataframe called a 'groupby object' as punishment.\n",
    "\"\"\"\n",
    "nests_raw = joined.groupby(['nest_id', 'datetime', 'temp_c', 'humidity']).count() \n",
    "print('Number of nest_ids:          {0:>20,}'.format(len(gb)))\n",
    "#nests = pd.DataFrame({'count' : joined.groupby( ['nest_id'] ).size()}).reset_index()\n",
    "nests_raw.head(10)\n",
    "# joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring nests dataframe to DB..\n"
     ]
    }
   ],
   "source": [
    "#sending temperature dataframe to the postgres DB\n",
    "print(\"Transferring temperature dataframe to DB..\")\n",
    "df_temp.to_sql(con=engine, name='penguins_temperature', if_exists='replace')\n",
    "print(\"Uploaded successfully\")\n",
    "\n",
    "#sending humidity dataframe to the postgres DB\n",
    "print(\"Transferring humidity dataframe to DB..\")\n",
    "df_humd.to_sql(con=engine, name='penguins_humidity', if_exists='replace')\n",
    "print(\"Uploaded successfully\")\n",
    "\n",
    "#sending nests dataframe to the postgres DB\n",
    "print(\"Transferring nests dataframe to DB..\")\n",
    "nests_raw.to_sql(con=engine, name='penguins_nests', if_exists='replace')\n",
    "print(\"Uploaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
